{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# 필요한 패키지(라이브러리)를 구글 드라이버에 설치하였는가?\n",
        "IS_INSTALL_ENV = True\n",
        "IS_COLAB_ENV = True"
      ],
      "metadata": {
        "id": "s73XJAGCx8hU"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## pyTorch 환경설정"
      ],
      "metadata": {
        "id": "_IuJDzkmLkzX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, sys \n",
        "import torch\n",
        "# True가 나와야 cuda 환경을 사용할 수 있다.\n",
        "torch.cuda.is_available()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U-hHT9tFLJUP",
        "outputId": "7296bdad-d6ff-4667-c061-79e6b29efde3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if IS_COLAB_ENV == True:\n",
        "    from google.colab import drive  \n",
        "    drive.mount('/content/drive')   \n",
        "\n",
        "    # colab의 os 폴더 위치\n",
        "    ds_path = '/content/dataset' \n",
        "\n",
        "    # Google Drive에서 설치된 파일 위치 - 미리 해당 폴더를 생성해야 한다.\n",
        "    os.symlink('/content/drive/My Drive/Colab Notebooks/Project/AI 속기사/dataset/', ds_path)  \n",
        "\n",
        "    # colab의 os 폴더 위치\n",
        "    pg_path = '/content/packages' \n",
        "\n",
        "    # Google Drive에서 설치된 파일 위치 - 미리 해당 폴더를 생성해야 한다.\n",
        "    os.symlink('/content/drive/My Drive/Colab Notebooks/packages/', pg_path)  \n",
        "    sys.path.insert(0, pg_path)  \n",
        "\n",
        "    # 거대한 모델을 위한 경로\n",
        "    md_path = '/content/wav2vec2-large-xlsr-ko-demo' \n",
        "    os.symlink('/content/drive/My Drive/Colab Notebooks/Project/AI 속기사/wav2vec2-large-xlsr-ko-demo/', md_path)  \n",
        "\n",
        "\n",
        "    # colab에서는 jiwer는 미리 설치해도 계속 설치하라고 나옴 \n",
        "    !pip install jiwer\n",
        "    \n",
        "    if IS_INSTALL_ENV == False:\n",
        "    # !pip install transformers 대신 영구 설치\n",
        "    # os.symlink로 걸어놓은 위치에서 pg_path로 연결된 폴더에 영구 설치한다. 1번만 설치하면 2번 설치할 필요 없음.\n",
        "        !pip install --target=$pg_path datasets\n",
        "        !pip install --target=$pg_path transformers  \n",
        "\n",
        "\n",
        "    # /content/drive/My Drive/Colab Notebooks/Project/AI 속기사/dataset/KlecSpeech/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u0isYf84fJEo",
        "outputId": "905ef4a8-b528-4170-e26e-8b5f0b92d8e8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lLCZvETJuFaO"
      },
      "source": [
        "## Create Wav2Vec2CTCTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "vjRnu_OffC4Z"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tqdm\n",
        "import glob"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dir_path = ['./dataset/KlecSpeech/csv/', './dataset/csv/']\n",
        "file_prefix = ['klecspeech_ko_500', 'order_speech_ko1000']"
      ],
      "metadata": {
        "id": "EfU2iZlCE0KY"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모든 csv 파일 리스트를 가져온다.\n",
        "def get_all_filelist(dir_path):\n",
        "    csv_files = []\n",
        "    for i in range(len(dir_path)):\n",
        "        csv_list = os.listdir(dir_path[i])\n",
        "        for file in csv_list:\n",
        "            csv_files.append(dir_path[i]+file)\n",
        "    return csv_files"
      ],
      "metadata": {
        "id": "vLx5NuU6E1ve"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 결과를 확인한다.\n",
        "all_files = get_all_filelist(dir_path)"
      ],
      "metadata": {
        "id": "MaPK5zemE5qW"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get_filelist \n",
        "# start_idx부터 step_num 건수 만큼 파일리스트 추가\n",
        "def get_filelist(csv_list, start_idx, step_num):\n",
        "\n",
        "    end_idx = len(csv_list)\n",
        "    new_list = []\n",
        "    if len(csv_list) == 0:\n",
        "        print(\"array size is 0\")\n",
        "        return None\n",
        "    else:\n",
        "        if len(csv_list) < start_idx + step_num:\n",
        "            end_idx = len(csv_list)\n",
        "        else:\n",
        "            end_idx = start_idx + step_num\n",
        "\n",
        "        for i in range(start_idx, end_idx):\n",
        "            print(\"file appned : \", i, \" \",csv_list[i])\n",
        "            new_list.append(csv_list[i])\n",
        "        return new_list"
      ],
      "metadata": {
        "id": "KtL3k-83E6j-"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_datasets(files):\n",
        "    # files가 list로 되어 있으면 한번에 합쳐서 load해온다.\n",
        "    dataset = load_dataset('csv',data_files=files, split='train', sep='\\t')\n",
        "    return dataset"
      ],
      "metadata": {
        "id": "N2RLI6uwE9MW"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "chars_to_ignore_regex = '[\\,\\?\\.\\!\\-\\;\\:\\\"\\“\\%\\‘\\”\\�]'\n",
        "\n",
        "def remove_special_characters(batch):\n",
        "    batch[\"text\"] = re.sub(chars_to_ignore_regex, '', batch[\"text\"]).lower() + \" \"\n",
        "    return batch"
      ],
      "metadata": {
        "id": "XYXoxpMdGEU6"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "YJUUl9lnryU-"
      },
      "outputs": [],
      "source": [
        "from transformers import Wav2Vec2CTCTokenizer\n",
        "\n",
        "tokenizer = Wav2Vec2CTCTokenizer(\"./vocab.json\",\n",
        "                                 unk_token=\"[UNK]\",\n",
        "                                 pad_token=\"[PAD]\",\n",
        "                                 word_delimiter_token=\"|\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCtrdRsuuDix"
      },
      "source": [
        "## Create XLSR-Wav2Vec2 Feature Extractor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "O0vdrkhKr8D1"
      },
      "outputs": [],
      "source": [
        "from transformers import Wav2Vec2FeatureExtractor\n",
        "\n",
        "feature_extractor = Wav2Vec2FeatureExtractor(feature_size=1,\n",
        "                                             sampling_rate=16000,\n",
        "                                             padding_value=0.0,\n",
        "                                             do_normalize=True,\n",
        "                                             return_attention_mask=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "fY_Qm1dpsE78"
      },
      "outputs": [],
      "source": [
        "from transformers import Wav2Vec2Processor\n",
        "\n",
        "processor = Wav2Vec2Processor(feature_extractor=feature_extractor,\n",
        "                              tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SS24ESkKfC4h"
      },
      "source": [
        "## add audio array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "5iD0kgHufC4i"
      },
      "outputs": [],
      "source": [
        "# import librosa\n",
        "# def load_audio(batch):\n",
        "#     batch['array'], _ = librosa.load(batch['filename'],sr=16000)\n",
        "#     return batch\n",
        "\n",
        "# csv에 array를 미리 넣었다.\n",
        "def load_audio(batch):\n",
        "    batch['array'] = np.array(batch['array'][1:-1].split(',')).astype(np.float32)\n",
        "    return batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "AscOxzgUsb50"
      },
      "outputs": [],
      "source": [
        "def prepare_dataset(batch):\n",
        "\n",
        "    # batched output is \"un-batched\"\n",
        "    batch[\"input_values\"] = processor(batch[\"array\"], sampling_rate=16000).input_values[0]\n",
        "    \n",
        "    with processor.as_target_processor():\n",
        "        batch[\"labels\"] = processor(batch[\"text\"]).input_ids\n",
        "    return batch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "StCtCQxDtuY3"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "nTlewiOzsyqR"
      },
      "outputs": [],
      "source": [
        "from dataclasses import dataclass, field\n",
        "from typing import Any, Dict, List, Optional, Union\n",
        "\n",
        "@dataclass\n",
        "class DataCollatorCTCWithPadding:\n",
        "    \"\"\"\n",
        "    Data collator that will dynamically pad the inputs received.\n",
        "    Args:\n",
        "        processor (:class:`~transformers.Wav2Vec2Processor`)\n",
        "            The processor used for proccessing the data.\n",
        "        padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):\n",
        "            Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n",
        "            among:\n",
        "            * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
        "              sequence if provided).\n",
        "            * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n",
        "              maximum acceptable input length for the model if that argument is not provided.\n",
        "            * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n",
        "              different lengths).\n",
        "        max_length (:obj:`int`, `optional`):\n",
        "            Maximum length of the ``input_values`` of the returned list and optionally padding length (see above).\n",
        "        max_length_labels (:obj:`int`, `optional`):\n",
        "            Maximum length of the ``labels`` returned list and optionally padding length (see above).\n",
        "        pad_to_multiple_of (:obj:`int`, `optional`):\n",
        "            If set will pad the sequence to a multiple of the provided value.\n",
        "            This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability >=\n",
        "            7.5 (Volta).\n",
        "    \"\"\"\n",
        "\n",
        "    processor: Wav2Vec2Processor\n",
        "    padding: Union[bool, str] = True\n",
        "    max_length: Optional[int] = None\n",
        "    max_length_labels: Optional[int] = None\n",
        "    pad_to_multiple_of: Optional[int] = None\n",
        "    pad_to_multiple_of_labels: Optional[int] = None\n",
        "\n",
        "    def __call__(self, features: List[Dict[str, Union[List[int], torch.cuda.HalfTensor]]]) -> Dict[str, torch.cuda.HalfTensor]:\n",
        "        # split inputs and labels since they have to be of different lenghts and need\n",
        "        # different padding methods\n",
        "        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n",
        "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
        "\n",
        "        batch = self.processor.pad(\n",
        "            input_features,\n",
        "            padding=self.padding,\n",
        "            max_length=self.max_length,\n",
        "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "        with self.processor.as_target_processor():\n",
        "            labels_batch = self.processor.pad(\n",
        "                label_features,\n",
        "                padding=self.padding,\n",
        "                max_length=self.max_length_labels,\n",
        "                pad_to_multiple_of=self.pad_to_multiple_of_labels,\n",
        "                return_tensors=\"pt\",\n",
        "            )\n",
        "\n",
        "        # replace padding with -100 to ignore loss correctly\n",
        "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
        "\n",
        "        batch[\"labels\"] = labels\n",
        "\n",
        "        return batch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nUTqiRY-tpXZ"
      },
      "source": [
        "## Set-up Trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "03mNNUfNthSZ"
      },
      "outputs": [],
      "source": [
        "def compute_metrics(pred):\n",
        "    pred_logits = pred.predictions\n",
        "    pred_ids = np.argmax(pred_logits, axis=-1)\n",
        "\n",
        "    pred.label_ids[pred.label_ids == -100] = processor.tokenizer.pad_token_id\n",
        "\n",
        "    pred_str = processor.batch_decode(pred_ids)\n",
        "    # we do not want to group tokens when computing the metrics\n",
        "    label_str = processor.batch_decode(pred.label_ids, group_tokens=False)\n",
        "\n",
        "    wer = wer_metric.compute(predictions=pred_str, references=label_str)\n",
        "\n",
        "    return {\"wer\": wer}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-aJNQplvQgn"
      },
      "source": [
        "## TrainingArguments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "-LuuwoeMvOnV"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments\n",
        "from transformers import Trainer\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "  # output_dir=\"/content/gdrive/MyDrive/wav2vec2-large-xlsr-ko-demo\",\n",
        "  output_dir=\"./wav2vec2-large-xlsr-ko-demo\",\n",
        "  group_by_length=True,\n",
        "  per_device_train_batch_size=8, # 원래는 16인데 cuda memory가 15G를 요구하기 때문에 절반으로 줄였다.\n",
        "  gradient_accumulation_steps=2,\n",
        "  evaluation_strategy=\"steps\",\n",
        "  num_train_epochs=5,\n",
        "  fp16=True,\n",
        "  save_steps=100,\n",
        "  eval_steps=100,\n",
        "  logging_steps=10,\n",
        "  learning_rate=3e-4,\n",
        "  warmup_steps=500,\n",
        "  save_total_limit=2,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_pretrain_path = './wav2vec2-large-xlsr-ko-demo/'\n",
        "\n",
        "# pretrain 폴더에서 가장 마지막 폴더를 가져온다.\n",
        "def get_last_pretrain_model():\n",
        "    listdirs = os.listdir(model_pretrain_path)\n",
        "    model_pretain_list = [folder for folder in listdirs if folder.startswith(\"checkpoint-\")]\n",
        "    if len(model_pretain_list)==0:\n",
        "        print(\"facebook/wav2vec2-large-xlsr-53\")\n",
        "        return \"facebook/wav2vec2-large-xlsr-53\"\n",
        "    else:\n",
        "        path = model_pretrain_path + max(model_pretain_list)\n",
        "        print(path)\n",
        "        return path"
      ],
      "metadata": {
        "id": "m2XBiCYUVQrU"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset, load_metric, Audio\n",
        "from transformers import Wav2Vec2ForCTC\n",
        "\n",
        "from torch.utils.data import random_split\n",
        "import gc\n",
        "\n",
        "# 한 번 실행에 몇 개의 파일을 묶을 것인가?\n",
        "num_of_files = 2\n",
        "\n",
        "# load_metric : 이미 구현된 메트릭을 사용할 수 있는 함수\n",
        "wer_metric = load_metric(\"wer\")\n",
        "\n",
        "for i in range(0, len(all_files), num_of_files):\n",
        "    filelist = get_filelist(all_files, i, num_of_files)\n",
        "    print(filelist)\n",
        "    data_csv = load_datasets(filelist)\n",
        "\n",
        "    remove_spectial_char_data = data_csv.map(remove_special_characters)\n",
        "    # list를 dataframe으로 변경해주고, dataframe에서 Dataset으로 변경해준다.\n",
        "    # 기존 'array' type이 str이라서 문제가 발생된다.\n",
        "    audio_data = remove_spectial_char_data.map(load_audio)\n",
        "\n",
        "    # Dataset type이 맞는지 확인할 것\n",
        "    print(type(audio_data))\n",
        "    print(type(data_csv))\n",
        "\n",
        "    ## step : processing\n",
        "    # for문 안에서 num_proc를 1 이상으로 설정하면, raise PicklingError 발생. 원인은 아직 모름.\n",
        "    # for문 밖에서는 num_proc를 4로 설정해도 이상없음.\n",
        "    order_voice = audio_data.map(\n",
        "        prepare_dataset,\n",
        "        remove_columns=remove_spectial_char_data.column_names,\n",
        "        num_proc=1 # jupyter notebook에서는 에러발생 #colab 또는 하드웨어 준비된 환경에서 사용 가능한 옵션, 병렬로 함수를 실행할 수있다. \n",
        "    )\n",
        "\n",
        "    data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)\n",
        "    \n",
        "    model = Wav2Vec2ForCTC.from_pretrained(\n",
        "    get_last_pretrain_model(), # colab에서는 처음에 \"facebook/wav2vec2-large-xlsr-53\", \n",
        "    attention_dropout=0.1,\n",
        "    hidden_dropout=0.1,\n",
        "    feat_proj_dropout=0.0,\n",
        "    mask_time_prob=0.05,\n",
        "    layerdrop=0.1,\n",
        "    ctc_loss_reduction=\"mean\", \n",
        "    pad_token_id=processor.tokenizer.pad_token_id,\n",
        "    vocab_size=len(processor.tokenizer))\n",
        "    \n",
        "\n",
        "    # XLSR-Wav2Vec2의 첫 번째 구성 요소는 원시 음성 신호에서 음향적으로 의미가 있지만 \n",
        "    # 문맥적으로 독립적인 기능을 추출하는 데 사용되는 CNN 계층 스택으로 구성됩니다.\n",
        "    # 모델의 이 부분은 사전 교육 중에 이미 충분히 훈련되었으며 논문에 명시된 바와 같이 더 이상 미세 조정할 필요가 없습니다. \n",
        "    # 따라서 특징 추출 부분의 모든 파라미터에 대해 require_grad를 False로 설정할 수 있다.\n",
        "    model.freeze_feature_extractor()\n",
        "\n",
        "    # 메모리를 절약하기 위해 그라데이션 체크포인팅을 활성화\n",
        "    model.gradient_checkpointing_enable()\n",
        "\n",
        "    ds_size = len(order_voice)\n",
        "    train_size = int(ds_size*0.8)\n",
        "    val_size = ds_size - train_size\n",
        "    train_ds, val_ds = random_split(order_voice,[train_size,val_size])\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        data_collator=data_collator,\n",
        "        args=training_args,\n",
        "        compute_metrics=compute_metrics,\n",
        "        train_dataset=train_ds,\n",
        "        eval_dataset=val_ds,\n",
        "        tokenizer=processor.feature_extractor,\n",
        "    )\n",
        "\n",
        "    # cuda cache를 초기화 한다.\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "    # RuntimeError: CUDA out of memory가 발생한다면, per_device_train_batch_size를 줄여본다.\n",
        "    # 단 batch_size를 줄이는 경우, 학습이 늦어지고, 학습률에 영향이 있을 수 있다.\n",
        "    trainer.train()\n"
      ],
      "metadata": {
        "id": "cwHYhOjWgYHO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "e137345f4c3e49a7a51240500884ac62",
            "d2758b4130394d69bb4d65e7aa338daf",
            "893f0fe7ac8a49508ade322ce8d1c3f8",
            "865508aa25a941caa098bfedf175a3c7",
            "f59d4780937746c0a048624c6468eb04",
            "b44a5e9aeec74a79863f70898858e61c",
            "96aaebb1f0264fbaab88465416e1f660",
            "922c79bdd4b3451fa8b6bf7087d6f42c",
            "eb93e84b7f5949ee888f024babc3d248",
            "852d7f973dde45a5a498de418036ef80",
            "61ff1814cb3145308bf12894f96241d4",
            "09cf305ccb134ee88e9c0890a3efb718",
            "c47fb46882454720804423c9690965f1",
            "bc22a5da21b2486ebb9c0f3e421a6323",
            "3a19c9bb8d614bd5a8bdd2975d4832fb",
            "ffa2fec0ab864de6a276a213e2193613",
            "ceb08b03a8ba4b4f8d58ffd1f263ef36",
            "b69717a712fd4db7a4cb584d2a8e4a2a",
            "6922ca8eb71c49c2b61dd1cf3d5add14",
            "00d22e57a7aa40ffb1ae8c377581f5b1",
            "21dc1d23a418478db715f399a703af57",
            "d6720294bad447fa941ec92528e0946e",
            "0e8bb145d2b2473bb5c742119c258ccf",
            "c98756ceb27e482687ec21be346f983c",
            "1255e1064f234360b1b4341d7e9fc573",
            "86ab664911d34f3d9224e66f916dc5cd",
            "7ea8a0cdd40746768bcf96aef7e97bf4",
            "f2a012e803ef45e18e8e64b66b61f4df",
            "4b2b546c45c1481387ba8480b5ff1123",
            "5fa90a8cf63c4326bb4198da021c9446",
            "058fa396e347436caee91186f15c0329",
            "1764d70f477443529a7723baaf4f37f7",
            "4fa18c5ca8544da1af74736dcc0c7e9b"
          ]
        },
        "outputId": "4737f928-56d3-4e6e-da5e-2dc10395590a"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using custom data configuration default-6202ba9b52256026\n",
            "Reusing dataset csv (/root/.cache/huggingface/datasets/csv/default-6202ba9b52256026/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "file appned :  0   ./dataset/KlecSpeech/csv/klecspeech_ko_500_000.csv\n",
            "file appned :  1   ./dataset/KlecSpeech/csv/klecspeech_ko_500_001.csv\n",
            "['./dataset/KlecSpeech/csv/klecspeech_ko_500_000.csv', './dataset/KlecSpeech/csv/klecspeech_ko_500_001.csv']\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/20 [00:00<?, ?ex/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e137345f4c3e49a7a51240500884ac62"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/20 [00:00<?, ?ex/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "09cf305ccb134ee88e9c0890a3efb718"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'datasets.arrow_dataset.Dataset'>\n",
            "<class 'datasets.arrow_dataset.Dataset'>\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/20 [00:00<?, ?ex/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0e8bb145d2b2473bb5c742119c258ccf"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset({\n",
            "    features: ['input_values', 'labels'],\n",
            "    num_rows: 20\n",
            "})\n",
            "./wav2vec2-large-xlsr-ko-demo/checkpoint-2000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading configuration file ./wav2vec2-large-xlsr-ko-demo/checkpoint-2000/config.json\n",
            "Model config Wav2Vec2Config {\n",
            "  \"_name_or_path\": \"./wav2vec2-large-xlsr-ko-demo/checkpoint-1900\",\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"adapter_kernel_size\": 3,\n",
            "  \"adapter_stride\": 2,\n",
            "  \"add_adapter\": false,\n",
            "  \"apply_spec_augment\": true,\n",
            "  \"architectures\": [\n",
            "    \"Wav2Vec2ForCTC\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"classifier_proj_size\": 256,\n",
            "  \"codevector_dim\": 768,\n",
            "  \"contrastive_logits_temperature\": 0.1,\n",
            "  \"conv_bias\": true,\n",
            "  \"conv_dim\": [\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512\n",
            "  ],\n",
            "  \"conv_kernel\": [\n",
            "    10,\n",
            "    3,\n",
            "    3,\n",
            "    3,\n",
            "    3,\n",
            "    2,\n",
            "    2\n",
            "  ],\n",
            "  \"conv_stride\": [\n",
            "    5,\n",
            "    2,\n",
            "    2,\n",
            "    2,\n",
            "    2,\n",
            "    2,\n",
            "    2\n",
            "  ],\n",
            "  \"ctc_loss_reduction\": \"mean\",\n",
            "  \"ctc_zero_infinity\": false,\n",
            "  \"diversity_loss_weight\": 0.1,\n",
            "  \"do_stable_layer_norm\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"feat_extract_activation\": \"gelu\",\n",
            "  \"feat_extract_dropout\": 0.0,\n",
            "  \"feat_extract_norm\": \"layer\",\n",
            "  \"feat_proj_dropout\": 0.0,\n",
            "  \"feat_quantizer_dropout\": 0.0,\n",
            "  \"final_dropout\": 0.0,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"layerdrop\": 0.1,\n",
            "  \"mask_channel_length\": 10,\n",
            "  \"mask_channel_min_space\": 1,\n",
            "  \"mask_channel_other\": 0.0,\n",
            "  \"mask_channel_prob\": 0.0,\n",
            "  \"mask_channel_selection\": \"static\",\n",
            "  \"mask_feature_length\": 10,\n",
            "  \"mask_feature_min_masks\": 0,\n",
            "  \"mask_feature_prob\": 0.0,\n",
            "  \"mask_time_length\": 10,\n",
            "  \"mask_time_min_masks\": 2,\n",
            "  \"mask_time_min_space\": 1,\n",
            "  \"mask_time_other\": 0.0,\n",
            "  \"mask_time_prob\": 0.05,\n",
            "  \"mask_time_selection\": \"static\",\n",
            "  \"model_type\": \"wav2vec2\",\n",
            "  \"num_adapter_layers\": 3,\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_codevector_groups\": 2,\n",
            "  \"num_codevectors_per_group\": 320,\n",
            "  \"num_conv_pos_embedding_groups\": 16,\n",
            "  \"num_conv_pos_embeddings\": 128,\n",
            "  \"num_feat_extract_layers\": 7,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"num_negatives\": 100,\n",
            "  \"output_hidden_size\": 1024,\n",
            "  \"pad_token_id\": 530,\n",
            "  \"proj_codevector_dim\": 768,\n",
            "  \"tdnn_dilation\": [\n",
            "    1,\n",
            "    2,\n",
            "    3,\n",
            "    1,\n",
            "    1\n",
            "  ],\n",
            "  \"tdnn_dim\": [\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    1500\n",
            "  ],\n",
            "  \"tdnn_kernel\": [\n",
            "    5,\n",
            "    3,\n",
            "    3,\n",
            "    1,\n",
            "    1\n",
            "  ],\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.19.2\",\n",
            "  \"use_weighted_layer_sum\": false,\n",
            "  \"vocab_size\": 531,\n",
            "  \"xvector_output_dim\": 512\n",
            "}\n",
            "\n",
            "loading weights file ./wav2vec2-large-xlsr-ko-demo/checkpoint-2000/pytorch_model.bin\n",
            "All model checkpoint weights were used when initializing Wav2Vec2ForCTC.\n",
            "\n",
            "All the weights of Wav2Vec2ForCTC were initialized from the model checkpoint at ./wav2vec2-large-xlsr-ko-demo/checkpoint-2000.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use Wav2Vec2ForCTC for predictions without further training.\n",
            "/content/packages/transformers/models/wav2vec2/modeling_wav2vec2.py:1677: FutureWarning: The method `freeze_feature_extractor` is deprecated and will be removed in Transformers v5.Please use the equivalent `freeze_feature_encoder` method instead.\n",
            "  FutureWarning,\n",
            "Using amp half precision backend\n",
            "/content/packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "***** Running training *****\n",
            "  Num examples = 16\n",
            "  Num Epochs = 5\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "  Gradient Accumulation steps = 2\n",
            "  Total optimization steps = 5\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [5/5 00:14, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Using custom data configuration default-ffe45d03b870e6b3\n",
            "Reusing dataset csv (/root/.cache/huggingface/datasets/csv/default-ffe45d03b870e6b3/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58)\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-ffe45d03b870e6b3/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58/cache-1c80317fa3b1799d.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-ffe45d03b870e6b3/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58/cache-bdd640fb06671ad1.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-ffe45d03b870e6b3/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58/cache-3eb13b9046685257.arrow\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "file appned :  2   ./dataset/csv/order_speech_ko1000_000.csv\n",
            "file appned :  3   ./dataset/csv/order_speech_ko1000_001.csv\n",
            "['./dataset/csv/order_speech_ko1000_000.csv', './dataset/csv/order_speech_ko1000_001.csv']\n",
            "<class 'datasets.arrow_dataset.Dataset'>\n",
            "<class 'datasets.arrow_dataset.Dataset'>\n",
            "Dataset({\n",
            "    features: ['input_values', 'labels'],\n",
            "    num_rows: 9\n",
            "})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading configuration file ./wav2vec2-large-xlsr-ko-demo/checkpoint-2000/config.json\n",
            "Model config Wav2Vec2Config {\n",
            "  \"_name_or_path\": \"./wav2vec2-large-xlsr-ko-demo/checkpoint-1900\",\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"adapter_kernel_size\": 3,\n",
            "  \"adapter_stride\": 2,\n",
            "  \"add_adapter\": false,\n",
            "  \"apply_spec_augment\": true,\n",
            "  \"architectures\": [\n",
            "    \"Wav2Vec2ForCTC\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"classifier_proj_size\": 256,\n",
            "  \"codevector_dim\": 768,\n",
            "  \"contrastive_logits_temperature\": 0.1,\n",
            "  \"conv_bias\": true,\n",
            "  \"conv_dim\": [\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512\n",
            "  ],\n",
            "  \"conv_kernel\": [\n",
            "    10,\n",
            "    3,\n",
            "    3,\n",
            "    3,\n",
            "    3,\n",
            "    2,\n",
            "    2\n",
            "  ],\n",
            "  \"conv_stride\": [\n",
            "    5,\n",
            "    2,\n",
            "    2,\n",
            "    2,\n",
            "    2,\n",
            "    2,\n",
            "    2\n",
            "  ],\n",
            "  \"ctc_loss_reduction\": \"mean\",\n",
            "  \"ctc_zero_infinity\": false,\n",
            "  \"diversity_loss_weight\": 0.1,\n",
            "  \"do_stable_layer_norm\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"feat_extract_activation\": \"gelu\",\n",
            "  \"feat_extract_dropout\": 0.0,\n",
            "  \"feat_extract_norm\": \"layer\",\n",
            "  \"feat_proj_dropout\": 0.0,\n",
            "  \"feat_quantizer_dropout\": 0.0,\n",
            "  \"final_dropout\": 0.0,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"layerdrop\": 0.1,\n",
            "  \"mask_channel_length\": 10,\n",
            "  \"mask_channel_min_space\": 1,\n",
            "  \"mask_channel_other\": 0.0,\n",
            "  \"mask_channel_prob\": 0.0,\n",
            "  \"mask_channel_selection\": \"static\",\n",
            "  \"mask_feature_length\": 10,\n",
            "  \"mask_feature_min_masks\": 0,\n",
            "  \"mask_feature_prob\": 0.0,\n",
            "  \"mask_time_length\": 10,\n",
            "  \"mask_time_min_masks\": 2,\n",
            "  \"mask_time_min_space\": 1,\n",
            "  \"mask_time_other\": 0.0,\n",
            "  \"mask_time_prob\": 0.05,\n",
            "  \"mask_time_selection\": \"static\",\n",
            "  \"model_type\": \"wav2vec2\",\n",
            "  \"num_adapter_layers\": 3,\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_codevector_groups\": 2,\n",
            "  \"num_codevectors_per_group\": 320,\n",
            "  \"num_conv_pos_embedding_groups\": 16,\n",
            "  \"num_conv_pos_embeddings\": 128,\n",
            "  \"num_feat_extract_layers\": 7,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"num_negatives\": 100,\n",
            "  \"output_hidden_size\": 1024,\n",
            "  \"pad_token_id\": 530,\n",
            "  \"proj_codevector_dim\": 768,\n",
            "  \"tdnn_dilation\": [\n",
            "    1,\n",
            "    2,\n",
            "    3,\n",
            "    1,\n",
            "    1\n",
            "  ],\n",
            "  \"tdnn_dim\": [\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    1500\n",
            "  ],\n",
            "  \"tdnn_kernel\": [\n",
            "    5,\n",
            "    3,\n",
            "    3,\n",
            "    1,\n",
            "    1\n",
            "  ],\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.19.2\",\n",
            "  \"use_weighted_layer_sum\": false,\n",
            "  \"vocab_size\": 531,\n",
            "  \"xvector_output_dim\": 512\n",
            "}\n",
            "\n",
            "loading weights file ./wav2vec2-large-xlsr-ko-demo/checkpoint-2000/pytorch_model.bin\n",
            "All model checkpoint weights were used when initializing Wav2Vec2ForCTC.\n",
            "\n",
            "All the weights of Wav2Vec2ForCTC were initialized from the model checkpoint at ./wav2vec2-large-xlsr-ko-demo/checkpoint-2000.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use Wav2Vec2ForCTC for predictions without further training.\n",
            "Using amp half precision backend\n",
            "***** Running training *****\n",
            "  Num examples = 7\n",
            "  Num Epochs = 5\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "  Gradient Accumulation steps = 2\n",
            "  Total optimization steps = 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "./wav2vec2-large-xlsr-ko-demo/checkpoint-2000\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [5/5 00:02, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "jslYUsZkOWpH"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "XLSR_Wav2Vec2_colab_recursive_train_20220617.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "STT",
      "language": "python",
      "name": "stt"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "99a2429cd063090ad39acbbb390da3f5879d741ee9cb02f1744db7376a74b1ea"
      }
    },
    "gpuClass": "standard",
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e137345f4c3e49a7a51240500884ac62": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d2758b4130394d69bb4d65e7aa338daf",
              "IPY_MODEL_893f0fe7ac8a49508ade322ce8d1c3f8",
              "IPY_MODEL_865508aa25a941caa098bfedf175a3c7"
            ],
            "layout": "IPY_MODEL_f59d4780937746c0a048624c6468eb04"
          }
        },
        "d2758b4130394d69bb4d65e7aa338daf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b44a5e9aeec74a79863f70898858e61c",
            "placeholder": "​",
            "style": "IPY_MODEL_96aaebb1f0264fbaab88465416e1f660",
            "value": "100%"
          }
        },
        "893f0fe7ac8a49508ade322ce8d1c3f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_922c79bdd4b3451fa8b6bf7087d6f42c",
            "max": 20,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_eb93e84b7f5949ee888f024babc3d248",
            "value": 20
          }
        },
        "865508aa25a941caa098bfedf175a3c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_852d7f973dde45a5a498de418036ef80",
            "placeholder": "​",
            "style": "IPY_MODEL_61ff1814cb3145308bf12894f96241d4",
            "value": " 20/20 [00:00&lt;00:00, 292.09ex/s]"
          }
        },
        "f59d4780937746c0a048624c6468eb04": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b44a5e9aeec74a79863f70898858e61c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "96aaebb1f0264fbaab88465416e1f660": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "922c79bdd4b3451fa8b6bf7087d6f42c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eb93e84b7f5949ee888f024babc3d248": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "852d7f973dde45a5a498de418036ef80": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "61ff1814cb3145308bf12894f96241d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "09cf305ccb134ee88e9c0890a3efb718": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c47fb46882454720804423c9690965f1",
              "IPY_MODEL_bc22a5da21b2486ebb9c0f3e421a6323",
              "IPY_MODEL_3a19c9bb8d614bd5a8bdd2975d4832fb"
            ],
            "layout": "IPY_MODEL_ffa2fec0ab864de6a276a213e2193613"
          }
        },
        "c47fb46882454720804423c9690965f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ceb08b03a8ba4b4f8d58ffd1f263ef36",
            "placeholder": "​",
            "style": "IPY_MODEL_b69717a712fd4db7a4cb584d2a8e4a2a",
            "value": "100%"
          }
        },
        "bc22a5da21b2486ebb9c0f3e421a6323": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6922ca8eb71c49c2b61dd1cf3d5add14",
            "max": 20,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_00d22e57a7aa40ffb1ae8c377581f5b1",
            "value": 20
          }
        },
        "3a19c9bb8d614bd5a8bdd2975d4832fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_21dc1d23a418478db715f399a703af57",
            "placeholder": "​",
            "style": "IPY_MODEL_d6720294bad447fa941ec92528e0946e",
            "value": " 20/20 [00:01&lt;00:00, 21.15ex/s]"
          }
        },
        "ffa2fec0ab864de6a276a213e2193613": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ceb08b03a8ba4b4f8d58ffd1f263ef36": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b69717a712fd4db7a4cb584d2a8e4a2a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6922ca8eb71c49c2b61dd1cf3d5add14": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "00d22e57a7aa40ffb1ae8c377581f5b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "21dc1d23a418478db715f399a703af57": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d6720294bad447fa941ec92528e0946e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0e8bb145d2b2473bb5c742119c258ccf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c98756ceb27e482687ec21be346f983c",
              "IPY_MODEL_1255e1064f234360b1b4341d7e9fc573",
              "IPY_MODEL_86ab664911d34f3d9224e66f916dc5cd"
            ],
            "layout": "IPY_MODEL_7ea8a0cdd40746768bcf96aef7e97bf4"
          }
        },
        "c98756ceb27e482687ec21be346f983c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f2a012e803ef45e18e8e64b66b61f4df",
            "placeholder": "​",
            "style": "IPY_MODEL_4b2b546c45c1481387ba8480b5ff1123",
            "value": "100%"
          }
        },
        "1255e1064f234360b1b4341d7e9fc573": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5fa90a8cf63c4326bb4198da021c9446",
            "max": 20,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_058fa396e347436caee91186f15c0329",
            "value": 20
          }
        },
        "86ab664911d34f3d9224e66f916dc5cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1764d70f477443529a7723baaf4f37f7",
            "placeholder": "​",
            "style": "IPY_MODEL_4fa18c5ca8544da1af74736dcc0c7e9b",
            "value": " 20/20 [00:00&lt;00:00, 24.71ex/s]"
          }
        },
        "7ea8a0cdd40746768bcf96aef7e97bf4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f2a012e803ef45e18e8e64b66b61f4df": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4b2b546c45c1481387ba8480b5ff1123": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5fa90a8cf63c4326bb4198da021c9446": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "058fa396e347436caee91186f15c0329": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1764d70f477443529a7723baaf4f37f7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4fa18c5ca8544da1af74736dcc0c7e9b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}