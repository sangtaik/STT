{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import json\n",
    "\n",
    "from unicode import split_syllables, join_jamos\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"text\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-8e0d0b8910acc620\n",
      "Reusing dataset csv (C:\\Users\\Mu-jun\\.cache\\huggingface\\datasets\\csv\\default-8e0d0b8910acc620\\0.0.0\\bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a)\n"
     ]
    }
   ],
   "source": [
    "all_data = load_dataset('csv',data_files='./order_speech_ko.csv',split='train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars_to_ignore_regex = '[\\,\\?\\.\\!\\-\\;\\:\\\"\\“\\%\\‘\\”\\�]'\n",
    "\n",
    "def remove_special_characters(batch):\n",
    "    batch[text] = re.sub(chars_to_ignore_regex, '', batch[text]).lower() + \" \"\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function remove_special_characters at 0x000001FE3DF02EE8> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dff826e3a9e04bac9d23b83e78c776d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/142367 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "remove_spectial_char_data = all_data.map(remove_special_characters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split to Font"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 142367/142367 [00:26<00:00, 5462.40it/s]\n"
     ]
    }
   ],
   "source": [
    "target_data = remove_spectial_char_data\n",
    "jamos = []\n",
    "for i in tqdm(range(len(target_data))):\n",
    "    split_text = split_syllables(target_data[i][text])\n",
    "    all_text = \" \".join(split_text)\n",
    "    jamos.extend(list(set(all_text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_list = list(set(jamos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_dict = {v: k+5 for k, v in enumerate(vocab_list)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Special Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_dict[\"|\"] = 4\n",
    "vocab_dict[\"<pad>\"] = 0\n",
    "vocab_dict[\"<s>\"] = 1\n",
    "vocab_dict[\"</s>\"] = 2\n",
    "vocab_dict[\"<unk>\"] = 3\n",
    "len(vocab_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ㅖ': 5,\n",
       " 'ㅙ': 6,\n",
       " 'ㄸ': 7,\n",
       " 'ㄿ': 8,\n",
       " 'ㅅ': 9,\n",
       " 'ㅝ': 10,\n",
       " 'ㅏ': 11,\n",
       " 't': 12,\n",
       " 'ㅜ': 13,\n",
       " 'ㄹ': 14,\n",
       " 'ㅛ': 15,\n",
       " 'ㅘ': 16,\n",
       " 'ㅡ': 17,\n",
       " 'ㅐ': 18,\n",
       " 'ㄲ': 19,\n",
       " 'ㄷ': 20,\n",
       " 'ㄺ': 21,\n",
       " 'ㅒ': 22,\n",
       " 'ㅎ': 23,\n",
       " 'c': 24,\n",
       " 'ㄴ': 25,\n",
       " 'ㅁ': 26,\n",
       " 'ㅉ': 27,\n",
       " 'ㅊ': 28,\n",
       " 'ㅈ': 29,\n",
       " 'ㄱ': 30,\n",
       " 'ㅄ': 31,\n",
       " 'ㄶ': 32,\n",
       " '5': 33,\n",
       " '3': 34,\n",
       " \"'\": 35,\n",
       " 'm': 36,\n",
       " 'ㅃ': 37,\n",
       " 'ㅀ': 38,\n",
       " 'ㅓ': 39,\n",
       " 'ㄼ': 40,\n",
       " 'ㅚ': 41,\n",
       " 'ㅠ': 42,\n",
       " 'ㅑ': 43,\n",
       " 'ㅇ': 44,\n",
       " 'ㅆ': 45,\n",
       " 'ㅕ': 46,\n",
       " 'ㅋ': 47,\n",
       " 'ㅍ': 48,\n",
       " 'v': 49,\n",
       " 'ㅣ': 50,\n",
       " 'ㅂ': 51,\n",
       " 'ㅢ': 52,\n",
       " 'b': 53,\n",
       " ' ': 54,\n",
       " 'ㅔ': 55,\n",
       " 'ㅟ': 56,\n",
       " '2': 57,\n",
       " 'ㅗ': 58,\n",
       " 'ㅌ': 59,\n",
       " '|': 4,\n",
       " '<pad>': 0,\n",
       " '<s>': 1,\n",
       " '</s>': 2,\n",
       " '<unk>': 3}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Vocabulary to json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('vocab_jamos.json', 'w') as vocab_file:\n",
    "    json.dump(vocab_dict, vocab_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ㄱ~ㅎ -> 12593 ~ 12622\n"
     ]
    }
   ],
   "source": [
    "print('ㄱ~ㅎ ->',ord('ㄱ'),'~',ord('ㅎ'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ㅏ~ㅣ-> 12623 ~ 12643\n"
     ]
    }
   ],
   "source": [
    "print('ㅏ~ㅣ->',ord('ㅏ'),'~',ord('ㅣ'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "가~힣 -> 44032 ~ 55203\n"
     ]
    }
   ],
   "source": [
    "print('가~힣 ->',ord('가'),'~',ord('힣'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def char_list(start_chr_num, end_chr_num):\n",
    "    return [chr(i) for i in range(start_chr_num,end_chr_num+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "51\n",
      "77\n",
      "103\n",
      "\n",
      "11275\n"
     ]
    }
   ],
   "source": [
    "jamos = []\n",
    "jamos.extend(char_list(ord('ㄱ'), ord('ㅎ')))\n",
    "print(len(jamos))\n",
    "\n",
    "jamos.extend(char_list(ord('ㅏ'), ord('ㅣ')))\n",
    "print(len(jamos))\n",
    "\n",
    "jamos.extend(char_list(ord('a'), ord('z')))\n",
    "print(len(jamos))\n",
    "\n",
    "jamos.extend(char_list(ord('A'), ord('Z')))\n",
    "print(len(jamos))\n",
    "\n",
    "print()\n",
    "\n",
    "chars = jamos.copy()\n",
    "chars.extend(char_list(ord('가'), ord('힣')))\n",
    "print(len(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unicode import split_syllables, join_jamos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<unk>'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_syllables('<unk>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "103"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_list = list(set(jamos))\n",
    "len(vocab_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_dict = {v: k+8 for k, v in enumerate(vocab_list)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "111"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_dict[\"<pad>\"] = 0\n",
    "vocab_dict[\"<s>\"] = 1\n",
    "vocab_dict[\"</s>\"] = 2\n",
    "vocab_dict[\"<unk>\"] = 3\n",
    "vocab_dict[\"|\"] = 4\n",
    "vocab_dict[\"<b>\"] = 5\n",
    "vocab_dict[\"<n>\"] = 6\n",
    "vocab_dict[\"<l>\"] = 7\n",
    "len(vocab_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('vocab_jamos.json', 'w') as vocab_file:\n",
    "    json.dump(vocab_dict, vocab_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11232"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_list = list(set(chars))\n",
    "vocab_dict = {v: k+8 for k, v in enumerate(vocab_list)}\n",
    "vocab_dict[\"<pad>\"] = 0\n",
    "vocab_dict[\"<s>\"] = 1\n",
    "vocab_dict[\"</s>\"] = 2\n",
    "vocab_dict[\"<unk>\"] = 3\n",
    "vocab_dict[\"|\"] = 4\n",
    "vocab_dict[\"<b>\"] = 5\n",
    "vocab_dict[\"<n>\"] = 6\n",
    "vocab_dict[\"<l>\"] = 7\n",
    "len(vocab_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('vocab_chars.json', 'w') as vocab_file:\n",
    "    json.dump(vocab_dict, vocab_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('STT')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "19631f30805cf65d5465564d75f0fe7c05dee5c1f7be198222dbe754da644e52"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
