{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lLCZvETJuFaO"
   },
   "source": [
    "## Create Wav2Vec2CTCTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets\n",
    "!pip install transformers # transformers는 최신 버전 회소 4.19.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-5004c11ebe3a2984\n",
      "Reusing dataset csv (C:\\Users\\sangt\\.cache\\huggingface\\datasets\\csv\\default-5004c11ebe3a2984\\0.0.0\\433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519)\n"
     ]
    }
   ],
   "source": [
    "# huggingface dataset\n",
    "all_data = load_dataset('csv',data_files='./order_speech_ko_test.csv',split='train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## add audio array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_audio(batch):\n",
    "#     batch['array'],_ = librosa.load('./dataset/audio/'+batch['src'],sr=16000)\n",
    "    batch['array'],_ = librosa.load(batch['src'], sr=16000)\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\sangt\\.cache\\huggingface\\datasets\\csv\\default-5004c11ebe3a2984\\0.0.0\\433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519\\cache-ef8f806719d74c07.arrow\n"
     ]
    }
   ],
   "source": [
    "all_data = all_data.map(load_audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.0015869140625,\n",
       " -0.001007080078125,\n",
       " -0.001129150390625,\n",
       " -0.00128173828125,\n",
       " -0.0009765625,\n",
       " -0.00042724609375,\n",
       " -0.000274658203125,\n",
       " -0.00164794921875,\n",
       " -0.00250244140625,\n",
       " -0.001861572265625,\n",
       " -0.00091552734375,\n",
       " -0.000640869140625,\n",
       " -0.000396728515625,\n",
       " 0.00048828125,\n",
       " 0.00103759765625,\n",
       " 0.00042724609375,\n",
       " 0.000274658203125,\n",
       " 0.0001220703125,\n",
       " 0.000335693359375,\n",
       " 0.000732421875,\n",
       " 0.000274658203125,\n",
       " 9.1552734375e-05,\n",
       " 0.000152587890625,\n",
       " 9.1552734375e-05,\n",
       " 3.0517578125e-05,\n",
       " 0.00048828125,\n",
       " 0.0003662109375,\n",
       " 0.000335693359375,\n",
       " 0.0015869140625,\n",
       " 0.0018310546875,\n",
       " 0.001739501953125,\n",
       " 0.00146484375,\n",
       " 0.001312255859375,\n",
       " 0.002044677734375,\n",
       " 0.00152587890625,\n",
       " 0.001190185546875,\n",
       " 0.0010986328125,\n",
       " 0.0003662109375,\n",
       " -6.103515625e-05,\n",
       " 0.0001220703125,\n",
       " -0.000274658203125,\n",
       " -0.00115966796875,\n",
       " -0.000732421875,\n",
       " -0.00128173828125,\n",
       " -0.001434326171875,\n",
       " -0.0018310546875,\n",
       " -0.00238037109375,\n",
       " -0.001007080078125,\n",
       " 0.000335693359375,\n",
       " 0.00042724609375,\n",
       " 0.000640869140625,\n",
       " 0.000640869140625,\n",
       " -6.103515625e-05,\n",
       " -0.00079345703125,\n",
       " -0.0010986328125,\n",
       " -0.00079345703125,\n",
       " -0.000732421875,\n",
       " 0.000152587890625,\n",
       " -0.0001220703125,\n",
       " -0.0010986328125,\n",
       " -0.00128173828125,\n",
       " -0.001373291015625,\n",
       " -0.0018310546875,\n",
       " -0.001434326171875,\n",
       " -0.0008544921875,\n",
       " -0.000518798828125,\n",
       " -0.000152587890625,\n",
       " -9.1552734375e-05,\n",
       " -0.001220703125,\n",
       " -0.001739501953125,\n",
       " -0.00115966796875,\n",
       " -0.000244140625,\n",
       " -0.00042724609375,\n",
       " -0.00128173828125,\n",
       " -0.00103759765625,\n",
       " -0.001861572265625,\n",
       " -0.002105712890625,\n",
       " -0.001861572265625,\n",
       " -0.001251220703125,\n",
       " -0.00140380859375,\n",
       " -0.0010986328125,\n",
       " -0.00146484375,\n",
       " -0.001556396484375,\n",
       " -0.002655029296875,\n",
       " -0.0015869140625,\n",
       " -0.000274658203125,\n",
       " 0.00030517578125,\n",
       " 0.000885009765625,\n",
       " 0.00054931640625,\n",
       " 0.00030517578125,\n",
       " -0.000518798828125,\n",
       " -0.00103759765625,\n",
       " -0.001434326171875,\n",
       " -0.001678466796875,\n",
       " -0.001312255859375,\n",
       " -0.001190185546875,\n",
       " -0.00140380859375,\n",
       " -0.001190185546875,\n",
       " -0.001495361328125,\n",
       " -0.0009765625,\n",
       " -0.001220703125,\n",
       " -0.00048828125,\n",
       " 0.000213623046875,\n",
       " 0.0003662109375,\n",
       " 0.00030517578125,\n",
       " 0.000396728515625,\n",
       " -0.0006103515625,\n",
       " -0.000885009765625,\n",
       " -0.000885009765625,\n",
       " -6.103515625e-05,\n",
       " -0.00030517578125,\n",
       " -0.0013427734375,\n",
       " -0.0009765625,\n",
       " -0.00189208984375,\n",
       " -0.00244140625,\n",
       " -0.001678466796875,\n",
       " -0.001434326171875,\n",
       " -0.0020751953125,\n",
       " -0.0015869140625,\n",
       " -0.0015869140625,\n",
       " -0.002593994140625,\n",
       " -0.00225830078125,\n",
       " -0.00091552734375,\n",
       " -0.00030517578125,\n",
       " -0.000152587890625,\n",
       " 0.000335693359375,\n",
       " 9.1552734375e-05,\n",
       " -0.000335693359375,\n",
       " -0.000823974609375,\n",
       " -0.000335693359375,\n",
       " -0.00103759765625,\n",
       " -0.000640869140625,\n",
       " -0.00042724609375,\n",
       " -0.0006103515625,\n",
       " -0.00128173828125,\n",
       " -0.001861572265625,\n",
       " -0.000732421875,\n",
       " -0.000396728515625,\n",
       " -0.00042724609375,\n",
       " 0.00048828125,\n",
       " 0.00103759765625,\n",
       " 0.00115966796875,\n",
       " 0.000457763671875,\n",
       " 0.00030517578125,\n",
       " 0.00042724609375,\n",
       " 0.00103759765625,\n",
       " 0.001007080078125,\n",
       " 0.00067138671875,\n",
       " 0.000244140625,\n",
       " -0.000457763671875,\n",
       " -0.000823974609375,\n",
       " -0.001007080078125,\n",
       " -0.000823974609375,\n",
       " -0.000885009765625,\n",
       " -0.000640869140625,\n",
       " -0.00048828125,\n",
       " -0.0003662109375,\n",
       " -0.00042724609375,\n",
       " -0.0010986328125,\n",
       " -0.0003662109375,\n",
       " 0.000518798828125,\n",
       " 0.001007080078125,\n",
       " 0.001434326171875,\n",
       " 0.001007080078125,\n",
       " 6.103515625e-05,\n",
       " -0.000518798828125,\n",
       " -0.0003662109375,\n",
       " 6.103515625e-05,\n",
       " -0.000244140625,\n",
       " -0.00030517578125,\n",
       " -6.103515625e-05,\n",
       " -0.00067138671875,\n",
       " -0.001953125,\n",
       " -0.001251220703125,\n",
       " -0.00128173828125,\n",
       " -0.0010986328125,\n",
       " -0.000946044921875,\n",
       " -6.103515625e-05,\n",
       " 0.000518798828125,\n",
       " 9.1552734375e-05,\n",
       " 0.000732421875,\n",
       " -0.001007080078125,\n",
       " -0.000152587890625,\n",
       " -0.000701904296875,\n",
       " -0.000244140625,\n",
       " -0.000885009765625,\n",
       " -0.00189208984375,\n",
       " -0.00091552734375,\n",
       " -0.001190185546875,\n",
       " -0.001251220703125,\n",
       " -0.001251220703125,\n",
       " -0.00128173828125,\n",
       " -0.0013427734375,\n",
       " -0.001739501953125,\n",
       " -0.001678466796875,\n",
       " -0.00128173828125,\n",
       " -0.0001220703125,\n",
       " 0.000457763671875,\n",
       " 0.00079345703125,\n",
       " 0.0009765625,\n",
       " 0.0003662109375,\n",
       " 0.001007080078125,\n",
       " -6.103515625e-05,\n",
       " -9.1552734375e-05,\n",
       " 0.000885009765625,\n",
       " 0.00091552734375,\n",
       " 0.0006103515625,\n",
       " 0.00048828125,\n",
       " 0.00054931640625,\n",
       " -0.000518798828125,\n",
       " -0.000579833984375,\n",
       " -0.000640869140625,\n",
       " 0.0003662109375,\n",
       " -0.00030517578125,\n",
       " -0.00048828125,\n",
       " 0.00030517578125,\n",
       " -0.000579833984375,\n",
       " -6.103515625e-05,\n",
       " 0.000518798828125,\n",
       " 0.000640869140625,\n",
       " 0.00079345703125,\n",
       " 0.0010986328125,\n",
       " 0.000946044921875,\n",
       " -0.00067138671875,\n",
       " -0.001007080078125,\n",
       " -0.00079345703125,\n",
       " -0.001068115234375,\n",
       " -0.00128173828125,\n",
       " -0.00128173828125,\n",
       " -0.000152587890625,\n",
       " -0.001617431640625,\n",
       " -0.0023193359375,\n",
       " -0.001190185546875,\n",
       " -0.000885009765625,\n",
       " 3.0517578125e-05,\n",
       " 0.00042724609375,\n",
       " 0.00054931640625,\n",
       " 0.00048828125,\n",
       " 0.000732421875,\n",
       " 0.001312255859375,\n",
       " 0.001068115234375,\n",
       " 0.000701904296875,\n",
       " 0.001861572265625,\n",
       " 0.0010986328125,\n",
       " 0.000213623046875,\n",
       " 0.00067138671875,\n",
       " 0.0001220703125,\n",
       " -0.000457763671875,\n",
       " -0.00030517578125,\n",
       " 0.000396728515625,\n",
       " 0.000457763671875,\n",
       " -0.000457763671875,\n",
       " 0.0001220703125,\n",
       " 0.000274658203125,\n",
       " -0.000579833984375,\n",
       " -0.0003662109375,\n",
       " 0.000762939453125,\n",
       " 0.001373291015625,\n",
       " 0.000823974609375,\n",
       " 0.000762939453125,\n",
       " -0.0001220703125,\n",
       " -0.000396728515625,\n",
       " -0.000335693359375,\n",
       " -0.000457763671875,\n",
       " -0.000732421875,\n",
       " -0.0009765625,\n",
       " 9.1552734375e-05,\n",
       " -0.00042724609375,\n",
       " -0.000885009765625,\n",
       " -0.001220703125,\n",
       " -0.00091552734375,\n",
       " -0.000335693359375,\n",
       " -6.103515625e-05,\n",
       " 0.000640869140625,\n",
       " 0.00140380859375,\n",
       " 0.00042724609375,\n",
       " 0.0010986328125,\n",
       " 0.00164794921875,\n",
       " 0.00091552734375,\n",
       " 0.001068115234375,\n",
       " 0.0015869140625,\n",
       " 0.001251220703125,\n",
       " 0.00054931640625,\n",
       " 0.00018310546875,\n",
       " -0.00030517578125,\n",
       " -0.0009765625,\n",
       " -0.000640869140625,\n",
       " -0.00030517578125,\n",
       " -0.00103759765625,\n",
       " -0.001434326171875,\n",
       " -0.0013427734375,\n",
       " -0.000274658203125,\n",
       " 0.00018310546875,\n",
       " -0.0003662109375,\n",
       " -0.00079345703125,\n",
       " -0.000335693359375,\n",
       " -0.0003662109375,\n",
       " -0.001220703125,\n",
       " -0.00103759765625,\n",
       " -0.00201416015625,\n",
       " -0.001220703125,\n",
       " 0.001190185546875,\n",
       " -0.003448486328125,\n",
       " -0.001739501953125,\n",
       " -0.001953125,\n",
       " -0.003662109375,\n",
       " -0.000213623046875,\n",
       " -0.00030517578125,\n",
       " -0.000946044921875,\n",
       " -0.000946044921875,\n",
       " -0.002227783203125,\n",
       " -0.00079345703125,\n",
       " -0.000457763671875,\n",
       " -0.0003662109375,\n",
       " 0.000274658203125,\n",
       " 6.103515625e-05,\n",
       " 0.001983642578125,\n",
       " 0.000457763671875,\n",
       " 0.00054931640625,\n",
       " 9.1552734375e-05,\n",
       " -0.0006103515625,\n",
       " 0.00018310546875,\n",
       " -0.000762939453125,\n",
       " -0.0008544921875,\n",
       " -0.000885009765625,\n",
       " -0.0006103515625,\n",
       " -0.00054931640625,\n",
       " -0.00177001953125,\n",
       " -0.001739501953125,\n",
       " -0.000518798828125,\n",
       " 0.0001220703125,\n",
       " 0.00091552734375,\n",
       " 0.001220703125,\n",
       " -6.103515625e-05,\n",
       " -6.103515625e-05,\n",
       " 0.0006103515625,\n",
       " -0.000640869140625,\n",
       " -0.0013427734375,\n",
       " -0.000244140625,\n",
       " -0.0003662109375,\n",
       " -0.001007080078125,\n",
       " -0.001495361328125,\n",
       " -0.0018310546875,\n",
       " -0.00189208984375,\n",
       " -0.001220703125,\n",
       " -0.000457763671875,\n",
       " -0.0008544921875,\n",
       " -0.001068115234375,\n",
       " -0.000701904296875,\n",
       " -0.00079345703125,\n",
       " -0.001190185546875,\n",
       " -0.001251220703125,\n",
       " -0.0003662109375,\n",
       " -0.00042724609375,\n",
       " 0.00018310546875,\n",
       " 0.000335693359375,\n",
       " -0.00018310546875,\n",
       " -0.00018310546875,\n",
       " -0.000518798828125,\n",
       " -0.00103759765625,\n",
       " -0.00128173828125,\n",
       " -0.00079345703125,\n",
       " -0.000274658203125,\n",
       " -0.0006103515625,\n",
       " -0.000457763671875,\n",
       " -0.000396728515625,\n",
       " -0.00103759765625,\n",
       " -0.00164794921875,\n",
       " -0.000732421875,\n",
       " -0.000640869140625,\n",
       " -0.000640869140625,\n",
       " -0.000732421875,\n",
       " -0.000457763671875,\n",
       " -0.00079345703125,\n",
       " -0.001251220703125,\n",
       " -0.0013427734375,\n",
       " -0.001861572265625,\n",
       " -0.001434326171875,\n",
       " -0.001678466796875,\n",
       " -0.001708984375,\n",
       " -0.001617431640625,\n",
       " -0.0010986328125,\n",
       " -0.000762939453125,\n",
       " -0.001068115234375,\n",
       " -0.001007080078125,\n",
       " -0.001190185546875,\n",
       " -0.00103759765625,\n",
       " -0.001617431640625,\n",
       " -0.001739501953125,\n",
       " -0.001922607421875,\n",
       " -0.001251220703125,\n",
       " -0.000518798828125,\n",
       " -0.0008544921875,\n",
       " 3.0517578125e-05,\n",
       " -0.0003662109375,\n",
       " -0.000579833984375,\n",
       " -0.000762939453125,\n",
       " -0.0018310546875,\n",
       " -0.00115966796875,\n",
       " -0.0003662109375,\n",
       " 0.000152587890625,\n",
       " -0.0006103515625,\n",
       " -0.000762939453125,\n",
       " -0.000701904296875,\n",
       " -0.000335693359375,\n",
       " -0.000274658203125,\n",
       " -0.00103759765625,\n",
       " -0.0008544921875,\n",
       " -0.000244140625,\n",
       " 0.000152587890625,\n",
       " -0.000396728515625,\n",
       " -0.0010986328125,\n",
       " -0.00018310546875,\n",
       " -0.000457763671875,\n",
       " -0.0010986328125,\n",
       " -0.0025634765625,\n",
       " -0.001861572265625,\n",
       " -0.00146484375,\n",
       " -0.000579833984375,\n",
       " -0.000274658203125,\n",
       " -0.000701904296875,\n",
       " -0.0001220703125,\n",
       " -0.000274658203125,\n",
       " -0.000274658203125,\n",
       " -0.000732421875,\n",
       " -0.000396728515625,\n",
       " -0.000335693359375,\n",
       " 0.000213623046875,\n",
       " 0.0003662109375,\n",
       " -0.000274658203125,\n",
       " 3.0517578125e-05,\n",
       " 0.000762939453125,\n",
       " 0.000396728515625,\n",
       " -0.000701904296875,\n",
       " -0.000701904296875,\n",
       " -0.00146484375,\n",
       " -0.000518798828125,\n",
       " -0.00042724609375,\n",
       " -0.00042724609375,\n",
       " 9.1552734375e-05,\n",
       " 0.000274658203125,\n",
       " 0.00115966796875,\n",
       " 0.000732421875,\n",
       " 0.00091552734375,\n",
       " 0.000244140625,\n",
       " 0.000274658203125,\n",
       " 0.0001220703125,\n",
       " 0.0006103515625,\n",
       " 0.000274658203125,\n",
       " -0.000518798828125,\n",
       " 0.000274658203125,\n",
       " -0.00030517578125,\n",
       " -0.00067138671875,\n",
       " -0.0015869140625,\n",
       " -0.002197265625,\n",
       " -0.00146484375,\n",
       " -0.001251220703125,\n",
       " -0.0006103515625,\n",
       " -0.00042724609375,\n",
       " -0.001251220703125,\n",
       " -0.00091552734375,\n",
       " -0.00067138671875,\n",
       " -0.000640869140625,\n",
       " -0.000823974609375,\n",
       " -0.000701904296875,\n",
       " -0.0003662109375,\n",
       " -0.00042724609375,\n",
       " -3.0517578125e-05,\n",
       " 0.000244140625,\n",
       " 9.1552734375e-05,\n",
       " -0.00042724609375,\n",
       " -0.000885009765625,\n",
       " 0.0003662109375,\n",
       " 0.000274658203125,\n",
       " 0.000274658203125,\n",
       " 0.001068115234375,\n",
       " 0.00048828125,\n",
       " -0.0003662109375,\n",
       " 0.0006103515625,\n",
       " 0.000885009765625,\n",
       " 0.001220703125,\n",
       " 0.001434326171875,\n",
       " 0.000823974609375,\n",
       " 0.0008544921875,\n",
       " 9.1552734375e-05,\n",
       " 0.000244140625,\n",
       " 0.00079345703125,\n",
       " 0.000518798828125,\n",
       " 0.0003662109375,\n",
       " -0.00067138671875,\n",
       " -0.001007080078125,\n",
       " -0.0008544921875,\n",
       " -0.00115966796875,\n",
       " -0.000579833984375,\n",
       " -0.000885009765625,\n",
       " -0.000518798828125,\n",
       " -6.103515625e-05,\n",
       " -0.000518798828125,\n",
       " -0.000579833984375,\n",
       " -0.000152587890625,\n",
       " -0.0003662109375,\n",
       " 0.0003662109375,\n",
       " 0.000823974609375,\n",
       " 0.000213623046875,\n",
       " -6.103515625e-05,\n",
       " 3.0517578125e-05,\n",
       " 0.00067138671875,\n",
       " 3.0517578125e-05,\n",
       " -3.0517578125e-05,\n",
       " -9.1552734375e-05,\n",
       " 0.000244140625,\n",
       " -0.000213623046875,\n",
       " -0.000457763671875,\n",
       " 0.001739501953125,\n",
       " -0.000823974609375,\n",
       " -0.00213623046875,\n",
       " 0.00152587890625,\n",
       " 0.001708984375,\n",
       " -0.0003662109375,\n",
       " 0.000335693359375,\n",
       " 0.00067138671875,\n",
       " 0.000335693359375,\n",
       " -0.00067138671875,\n",
       " -0.001068115234375,\n",
       " 0.001220703125,\n",
       " 0.000885009765625,\n",
       " -0.000946044921875,\n",
       " -6.103515625e-05,\n",
       " -0.000518798828125,\n",
       " -0.000946044921875,\n",
       " -0.00067138671875,\n",
       " -0.00048828125,\n",
       " 0.0003662109375,\n",
       " 0.000518798828125,\n",
       " 0.000579833984375,\n",
       " 0.000885009765625,\n",
       " 0.000396728515625,\n",
       " 9.1552734375e-05,\n",
       " 0.000732421875,\n",
       " 0.0010986328125,\n",
       " 0.00177001953125,\n",
       " 0.001434326171875,\n",
       " 0.0008544921875,\n",
       " 0.000701904296875,\n",
       " -0.000335693359375,\n",
       " -0.0001220703125,\n",
       " 6.103515625e-05,\n",
       " 0.00054931640625,\n",
       " 0.00067138671875,\n",
       " -0.000274658203125,\n",
       " -0.000213623046875,\n",
       " -0.000152587890625,\n",
       " -0.0001220703125,\n",
       " 0.000274658203125,\n",
       " 0.000762939453125,\n",
       " 0.0008544921875,\n",
       " 0.000518798828125,\n",
       " 0.000885009765625,\n",
       " 0.00067138671875,\n",
       " 0.00018310546875,\n",
       " 0.000274658203125,\n",
       " 0.000701904296875,\n",
       " 0.0009765625,\n",
       " 0.0015869140625,\n",
       " 0.000823974609375,\n",
       " 0.0008544921875,\n",
       " 0.00030517578125,\n",
       " -0.00018310546875,\n",
       " -0.000701904296875,\n",
       " -0.000396728515625,\n",
       " -6.103515625e-05,\n",
       " 0.000701904296875,\n",
       " 0.000518798828125,\n",
       " 0.000244140625,\n",
       " -0.000457763671875,\n",
       " -0.00067138671875,\n",
       " 0.0,\n",
       " 0.00067138671875,\n",
       " 0.00091552734375,\n",
       " 0.0003662109375,\n",
       " 3.0517578125e-05,\n",
       " -3.0517578125e-05,\n",
       " 0.000244140625,\n",
       " -6.103515625e-05,\n",
       " 0.0,\n",
       " 9.1552734375e-05,\n",
       " 0.000213623046875,\n",
       " 3.0517578125e-05,\n",
       " 3.0517578125e-05,\n",
       " -0.00054931640625,\n",
       " -0.000396728515625,\n",
       " 0.0006103515625,\n",
       " 0.001312255859375,\n",
       " 0.0013427734375,\n",
       " 0.000732421875,\n",
       " -6.103515625e-05,\n",
       " 0.00042724609375,\n",
       " 0.00018310546875,\n",
       " 9.1552734375e-05,\n",
       " 0.0,\n",
       " 0.0003662109375,\n",
       " 0.0001220703125,\n",
       " -0.00054931640625,\n",
       " -0.000335693359375,\n",
       " -0.000457763671875,\n",
       " -0.001312255859375,\n",
       " -0.00042724609375,\n",
       " -6.103515625e-05,\n",
       " 0.0,\n",
       " 0.0008544921875,\n",
       " -0.0001220703125,\n",
       " -0.00042724609375,\n",
       " -0.0008544921875,\n",
       " -0.0001220703125,\n",
       " 0.001373291015625,\n",
       " -0.000396728515625,\n",
       " 0.000274658203125,\n",
       " -0.000396728515625,\n",
       " -0.000762939453125,\n",
       " 0.000396728515625,\n",
       " 9.1552734375e-05,\n",
       " 0.00048828125,\n",
       " -0.000640869140625,\n",
       " -0.00067138671875,\n",
       " -0.000213623046875,\n",
       " -0.000946044921875,\n",
       " -0.00115966796875,\n",
       " -0.00054931640625,\n",
       " -0.0006103515625,\n",
       " -0.000762939453125,\n",
       " 0.000396728515625,\n",
       " 0.0,\n",
       " -0.00048828125,\n",
       " -0.0003662109375,\n",
       " -0.001312255859375,\n",
       " -0.00042724609375,\n",
       " -3.0517578125e-05,\n",
       " 0.001007080078125,\n",
       " 0.000244140625,\n",
       " -0.0009765625,\n",
       " -0.000244140625,\n",
       " 0.00030517578125,\n",
       " 3.0517578125e-05,\n",
       " 3.0517578125e-05,\n",
       " 0.001129150390625,\n",
       " 0.001190185546875,\n",
       " 0.0003662109375,\n",
       " 0.00018310546875,\n",
       " 0.000396728515625,\n",
       " 3.0517578125e-05,\n",
       " 0.000335693359375,\n",
       " -0.000152587890625,\n",
       " 0.00048828125,\n",
       " 0.000762939453125,\n",
       " 0.001007080078125,\n",
       " 0.00103759765625,\n",
       " -6.103515625e-05,\n",
       " 9.1552734375e-05,\n",
       " 0.0006103515625,\n",
       " -0.000885009765625,\n",
       " -0.000274658203125,\n",
       " 0.000640869140625,\n",
       " 0.000152587890625,\n",
       " -0.000762939453125,\n",
       " -0.000823974609375,\n",
       " -0.0003662109375,\n",
       " 3.0517578125e-05,\n",
       " 0.000274658203125,\n",
       " 0.0006103515625,\n",
       " 0.00048828125,\n",
       " -0.0008544921875,\n",
       " -0.001129150390625,\n",
       " -0.00079345703125,\n",
       " 6.103515625e-05,\n",
       " -0.00030517578125,\n",
       " 0.000335693359375,\n",
       " 0.000762939453125,\n",
       " -0.000213623046875,\n",
       " -0.000518798828125,\n",
       " -0.00054931640625,\n",
       " -0.000244140625,\n",
       " -0.00079345703125,\n",
       " -0.000518798828125,\n",
       " -0.000885009765625,\n",
       " -0.000579833984375,\n",
       " -0.000579833984375,\n",
       " -0.000640869140625,\n",
       " 0.00030517578125,\n",
       " -0.000732421875,\n",
       " -0.000457763671875,\n",
       " 0.00018310546875,\n",
       " -0.00018310546875,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.00048828125,\n",
       " -0.000152587890625,\n",
       " -0.000274658203125,\n",
       " 0.0,\n",
       " -0.00048828125,\n",
       " -0.000701904296875,\n",
       " -0.0018310546875,\n",
       " -0.00152587890625,\n",
       " -0.0010986328125,\n",
       " -0.000946044921875,\n",
       " -0.000244140625,\n",
       " 0.000396728515625,\n",
       " 0.00079345703125,\n",
       " 0.00030517578125,\n",
       " -0.00042724609375,\n",
       " -0.00048828125,\n",
       " -0.0001220703125,\n",
       " 0.0003662109375,\n",
       " 0.000335693359375,\n",
       " 0.0001220703125,\n",
       " 0.000213623046875,\n",
       " 0.0,\n",
       " -0.00018310546875,\n",
       " -0.000885009765625,\n",
       " -0.0006103515625,\n",
       " -0.00079345703125,\n",
       " -0.0009765625,\n",
       " -0.0003662109375,\n",
       " 0.0,\n",
       " 0.00018310546875,\n",
       " 0.0,\n",
       " 0.0009765625,\n",
       " 0.000244140625,\n",
       " 0.00030517578125,\n",
       " 0.000274658203125,\n",
       " 6.103515625e-05,\n",
       " 0.000732421875,\n",
       " 0.000640869140625,\n",
       " 0.000823974609375,\n",
       " -6.103515625e-05,\n",
       " -0.000518798828125,\n",
       " 0.000213623046875,\n",
       " 0.00048828125,\n",
       " -0.000518798828125,\n",
       " -0.000457763671875,\n",
       " 0.000244140625,\n",
       " 0.000762939453125,\n",
       " 0.000579833984375,\n",
       " 0.00067138671875,\n",
       " -9.1552734375e-05,\n",
       " 0.00067138671875,\n",
       " 0.00067138671875,\n",
       " -0.000823974609375,\n",
       " 0.00030517578125,\n",
       " 0.0009765625,\n",
       " -0.000274658203125,\n",
       " 0.00128173828125,\n",
       " 0.00079345703125,\n",
       " 0.0001220703125,\n",
       " 0.000518798828125,\n",
       " 0.000335693359375,\n",
       " 0.000885009765625,\n",
       " -3.0517578125e-05,\n",
       " 0.000396728515625,\n",
       " 0.001068115234375,\n",
       " 0.000518798828125,\n",
       " 0.000335693359375,\n",
       " 0.00091552734375,\n",
       " 0.00128173828125,\n",
       " 0.00030517578125,\n",
       " 0.001495361328125,\n",
       " 0.001373291015625,\n",
       " 0.000946044921875,\n",
       " 0.001251220703125,\n",
       " 0.00091552734375,\n",
       " 0.000732421875,\n",
       " 0.00018310546875,\n",
       " -0.000396728515625,\n",
       " -0.0006103515625,\n",
       " -0.001129150390625,\n",
       " -0.0008544921875,\n",
       " -0.0006103515625,\n",
       " -0.000885009765625,\n",
       " -0.0003662109375,\n",
       " -9.1552734375e-05,\n",
       " -9.1552734375e-05,\n",
       " -0.000518798828125,\n",
       " -0.001068115234375,\n",
       " -0.0009765625,\n",
       " 0.00042724609375,\n",
       " 0.0003662109375,\n",
       " 0.000732421875,\n",
       " 0.00048828125,\n",
       " 0.000274658203125,\n",
       " 0.000762939453125,\n",
       " 0.001495361328125,\n",
       " 0.00103759765625,\n",
       " 0.00054931640625,\n",
       " 0.00128173828125,\n",
       " 0.000762939453125,\n",
       " 0.000946044921875,\n",
       " -6.103515625e-05,\n",
       " -0.000274658203125,\n",
       " 0.000152587890625,\n",
       " 0.000274658203125,\n",
       " 0.00042724609375,\n",
       " 0.00042724609375,\n",
       " 0.000823974609375,\n",
       " 0.000946044921875,\n",
       " 0.000518798828125,\n",
       " 0.000457763671875,\n",
       " 0.000274658203125,\n",
       " -0.000579833984375,\n",
       " -0.000732421875,\n",
       " -0.00067138671875,\n",
       " -6.103515625e-05,\n",
       " -0.00079345703125,\n",
       " -0.000579833984375,\n",
       " -0.000579833984375,\n",
       " -0.000701904296875,\n",
       " -0.000274658203125,\n",
       " -0.000244140625,\n",
       " -0.000335693359375,\n",
       " -3.0517578125e-05,\n",
       " 0.000518798828125,\n",
       " 0.0003662109375,\n",
       " 0.0009765625,\n",
       " 0.001739501953125,\n",
       " 0.001434326171875,\n",
       " 0.001129150390625,\n",
       " 0.0010986328125,\n",
       " 0.0015869140625,\n",
       " 0.001129150390625,\n",
       " 0.00079345703125,\n",
       " 0.000885009765625,\n",
       " 0.000701904296875,\n",
       " 0.000152587890625,\n",
       " 0.000335693359375,\n",
       " 0.000762939453125,\n",
       " -3.0517578125e-05,\n",
       " 0.0001220703125,\n",
       " 0.000213623046875,\n",
       " 0.000335693359375,\n",
       " 0.0008544921875,\n",
       " 0.0009765625,\n",
       " 0.000213623046875,\n",
       " 0.0003662109375,\n",
       " 0.0009765625,\n",
       " 0.000946044921875,\n",
       " 0.001251220703125,\n",
       " 0.000335693359375,\n",
       " 6.103515625e-05,\n",
       " -9.1552734375e-05,\n",
       " -0.000152587890625,\n",
       " 0.00018310546875,\n",
       " -0.000335693359375,\n",
       " -0.000152587890625,\n",
       " -0.000244140625,\n",
       " -0.0003662109375,\n",
       " -0.0008544921875,\n",
       " -0.001373291015625,\n",
       " -0.000457763671875,\n",
       " -0.000762939453125,\n",
       " -0.000244140625,\n",
       " 0.00042724609375,\n",
       " 0.000701904296875,\n",
       " 0.00103759765625,\n",
       " 0.000701904296875,\n",
       " 0.00177001953125,\n",
       " 0.001678466796875,\n",
       " 0.0013427734375,\n",
       " 0.000335693359375,\n",
       " 0.00128173828125,\n",
       " 0.001953125,\n",
       " 0.000335693359375,\n",
       " 0.0006103515625,\n",
       " 0.0003662109375,\n",
       " 0.001129150390625,\n",
       " -0.00054931640625,\n",
       " -0.0008544921875,\n",
       " -0.00030517578125,\n",
       " 0.0001220703125,\n",
       " 0.00054931640625,\n",
       " 0.000396728515625,\n",
       " 0.000335693359375,\n",
       " 0.000640869140625,\n",
       " 0.000335693359375,\n",
       " -9.1552734375e-05,\n",
       " -0.0003662109375,\n",
       " -0.00030517578125,\n",
       " -0.00115966796875,\n",
       " -0.00067138671875,\n",
       " -0.001678466796875,\n",
       " -0.00115966796875,\n",
       " -0.00140380859375,\n",
       " -0.000885009765625,\n",
       " -0.000885009765625,\n",
       " -0.00018310546875,\n",
       " -0.0006103515625,\n",
       " -0.001373291015625,\n",
       " -0.0003662109375,\n",
       " -0.000579833984375,\n",
       " 0.000213623046875,\n",
       " 0.0,\n",
       " 0.000640869140625,\n",
       " 0.001068115234375,\n",
       " 0.0009765625,\n",
       " 0.00128173828125,\n",
       " 0.000762939453125,\n",
       " -9.1552734375e-05,\n",
       " 0.000579833984375,\n",
       " 0.000640869140625,\n",
       " 0.0006103515625,\n",
       " -0.00018310546875,\n",
       " 0.000213623046875,\n",
       " -0.00018310546875,\n",
       " -0.0008544921875,\n",
       " -0.0009765625,\n",
       " -0.0006103515625,\n",
       " -0.000213623046875,\n",
       " -0.000244140625,\n",
       " -0.0006103515625,\n",
       " -6.103515625e-05,\n",
       " -0.000762939453125,\n",
       " -0.000701904296875,\n",
       " -0.0008544921875,\n",
       " -0.0010986328125,\n",
       " -0.001068115234375,\n",
       " -0.001556396484375,\n",
       " -0.001251220703125,\n",
       " -0.00079345703125,\n",
       " 0.0001220703125,\n",
       " 6.103515625e-05,\n",
       " -0.000457763671875,\n",
       " -0.00103759765625,\n",
       " -0.001068115234375,\n",
       " -0.0006103515625,\n",
       " -0.000518798828125,\n",
       " 0.000701904296875,\n",
       " 0.00048828125,\n",
       " 0.000518798828125,\n",
       " 0.000762939453125,\n",
       " 0.0006103515625,\n",
       " 0.00067138671875,\n",
       " 0.00042724609375,\n",
       " 0.00079345703125,\n",
       " 9.1552734375e-05,\n",
       " 0.000335693359375,\n",
       " 0.0001220703125,\n",
       " 9.1552734375e-05,\n",
       " 9.1552734375e-05,\n",
       " -3.0517578125e-05,\n",
       " 0.000244140625,\n",
       " 0.000152587890625,\n",
       " -6.103515625e-05,\n",
       " 3.0517578125e-05,\n",
       " -0.0009765625,\n",
       " -0.0013427734375,\n",
       " -0.0009765625,\n",
       " -0.00091552734375,\n",
       " -0.0008544921875,\n",
       " -0.00054931640625,\n",
       " -0.00067138671875,\n",
       " -0.001220703125,\n",
       " -0.0013427734375,\n",
       " -0.001373291015625,\n",
       " -0.000701904296875,\n",
       " -0.00079345703125,\n",
       " -0.000640869140625,\n",
       " -0.00030517578125,\n",
       " -0.000274658203125,\n",
       " 0.00018310546875,\n",
       " 9.1552734375e-05,\n",
       " 0.0001220703125,\n",
       " 6.103515625e-05,\n",
       " -0.000244140625,\n",
       " 0.0006103515625,\n",
       " 0.000518798828125,\n",
       " 0.00054931640625,\n",
       " 0.001068115234375,\n",
       " 0.000762939453125,\n",
       " 0.0006103515625,\n",
       " 0.00103759765625,\n",
       " 0.001068115234375,\n",
       " 0.000335693359375,\n",
       " 0.000244140625,\n",
       " -3.0517578125e-05,\n",
       " -0.00042724609375,\n",
       " 0.0003662109375,\n",
       " -0.0008544921875,\n",
       " -0.00030517578125,\n",
       " -0.000152587890625,\n",
       " -0.000244140625,\n",
       " 0.000396728515625,\n",
       " -0.000823974609375,\n",
       " -0.000579833984375,\n",
       " 0.0008544921875,\n",
       " 0.0009765625,\n",
       " 0.000518798828125,\n",
       " -3.0517578125e-05,\n",
       " -0.000396728515625,\n",
       " -0.000244140625,\n",
       " -0.00079345703125,\n",
       " -0.0010986328125,\n",
       " -0.000762939453125,\n",
       " -0.000946044921875,\n",
       " ...]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터 확인\n",
    "all_data[0]['array'][1*16000:2*16000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(301, 5)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Unnamed: 0': 0,\n",
       " 'src': './dataset/KlecSpeech/Validation/D01/E01/S000028/000000.wav',\n",
       " 'text': '여러분 안녕하세요',\n",
       " 'text_len': 9,\n",
       " 'array': [0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  ...]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(all_data.shape)\n",
    "all_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "G34Hj6BgnK3L"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "chars_to_ignore_regex = '[\\,\\?\\.\\!\\-\\;\\:\\\"\\“\\%\\‘\\”\\�]'\n",
    "\n",
    "def remove_sepcial_characters(batch):\n",
    "    try:\n",
    "        debug = batch[\"text\"]\n",
    "        batch[\"text\"] = re.sub(chars_to_ignore_regex, '', batch[\"text\"]).lower() + \" \"\n",
    "    except TypeError as e:\n",
    "        print(debug, \"Not a valid choice! Try again : \", e)\n",
    "        \n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\sangt\\.cache\\huggingface\\datasets\\csv\\default-5004c11ebe3a2984\\0.0.0\\433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519\\cache-f90d273093fcc226.arrow\n"
     ]
    }
   ],
   "source": [
    "#TPU나 GPU없으면 1시간 정도 걸림 32971건기준\n",
    "# 메모리 부족으로 나눠서 진행 필요. (16GB 이상)\n",
    "remove_spectial_char_data = all_data.map(remove_sepcial_characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Unnamed: 0', 'src', 'text', 'text_len', 'array'],\n",
       "    num_rows: 301\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_spectial_char_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>src</th>\n",
       "      <th>text</th>\n",
       "      <th>text_len</th>\n",
       "      <th>array</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>./dataset/KlecSpeech/Validation/D01/E01/S00002...</td>\n",
       "      <td>여러분 안녕하세요</td>\n",
       "      <td>9</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>./dataset/KlecSpeech/Validation/D01/E01/S00002...</td>\n",
       "      <td>친구들이 독해의 열매를 맺어 독해력이 쑥쑥 자랄 수 있도록 도와주는 혜은 선생님 이...</td>\n",
       "      <td>83</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>./dataset/KlecSpeech/Validation/D01/E01/S00002...</td>\n",
       "      <td>오늘의 생각 씨앗은요 히읗 비읍 이에요 선생님은 어렸을 때 이 생각 씨앗을 입고 싶...</td>\n",
       "      <td>120</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>./dataset/KlecSpeech/Validation/D01/E01/S00002...</td>\n",
       "      <td>색동저고리 함께 읽어보도록 하겠습니다</td>\n",
       "      <td>20</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>./dataset/KlecSpeech/Validation/D01/E01/S00002...</td>\n",
       "      <td>와 엄마 너무 예뻐요</td>\n",
       "      <td>11</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                                src  \\\n",
       "0           0  ./dataset/KlecSpeech/Validation/D01/E01/S00002...   \n",
       "1           1  ./dataset/KlecSpeech/Validation/D01/E01/S00002...   \n",
       "2           2  ./dataset/KlecSpeech/Validation/D01/E01/S00002...   \n",
       "3           3  ./dataset/KlecSpeech/Validation/D01/E01/S00002...   \n",
       "4           4  ./dataset/KlecSpeech/Validation/D01/E01/S00002...   \n",
       "\n",
       "                                                text  text_len  \\\n",
       "0                                         여러분 안녕하세요          9   \n",
       "1  친구들이 독해의 열매를 맺어 독해력이 쑥쑥 자랄 수 있도록 도와주는 혜은 선생님 이...        83   \n",
       "2  오늘의 생각 씨앗은요 히읗 비읍 이에요 선생님은 어렸을 때 이 생각 씨앗을 입고 싶...       120   \n",
       "3                              색동저고리 함께 읽어보도록 하겠습니다         20   \n",
       "4                                       와 엄마 너무 예뻐요         11   \n",
       "\n",
       "                                               array  \n",
       "0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "1  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "2  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "3  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "4  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 여기서 시간이 무진장 걸림 1000건 이상은 너무 오래 걸림(!시간 이상)\n",
    "df = pd.DataFrame(remove_spectial_char_data)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "ZsimiwGSnWDY"
   },
   "outputs": [],
   "source": [
    "def extract_all_chars(batch):\n",
    "  all_text = \" \".join(batch[\"text\"])\n",
    "  vocab = list(set(all_text))\n",
    "  return {\"vocab\": [vocab], \"all_text\": [all_text]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92fc805d3b6742b1ab3081959684c845",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "char_vocab = remove_spectial_char_data.map(\n",
    "    extract_all_chars,\n",
    "    batched=True,\n",
    "    batch_size=-1,\n",
    "    keep_in_memory=True,\n",
    "    remove_columns=remove_spectial_char_data.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['vocab', 'all_text'],\n",
       "    num_rows: 1\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_list = list(set(char_vocab[\"vocab\"][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'덩': 0,\n",
       " '더': 1,\n",
       " '우': 2,\n",
       " '렸': 3,\n",
       " '워': 4,\n",
       " '징': 5,\n",
       " '같': 6,\n",
       " '웃': 7,\n",
       " '이': 8,\n",
       " '품': 9,\n",
       " '급': 10,\n",
       " '몽': 11,\n",
       " '슨': 12,\n",
       " '됐': 13,\n",
       " '깔': 14,\n",
       " '식': 15,\n",
       " '뭔': 16,\n",
       " '련': 17,\n",
       " '들': 18,\n",
       " '오': 19,\n",
       " '앗': 20,\n",
       " '의': 21,\n",
       " '빔': 22,\n",
       " '해': 23,\n",
       " '꿈': 24,\n",
       " '록': 25,\n",
       " '게': 26,\n",
       " '석': 27,\n",
       " '마': 28,\n",
       " '직': 29,\n",
       " '다': 30,\n",
       " '릇': 31,\n",
       " '변': 32,\n",
       " '모': 33,\n",
       " '잃': 34,\n",
       " '커': 35,\n",
       " '끔': 36,\n",
       " '실': 37,\n",
       " '답': 38,\n",
       " '말': 39,\n",
       " '영': 40,\n",
       " '꿔': 41,\n",
       " '숲': 42,\n",
       " '쁜': 43,\n",
       " '투': 44,\n",
       " '끌': 45,\n",
       " '형': 46,\n",
       " '느': 47,\n",
       " '동': 48,\n",
       " '덕': 49,\n",
       " '맙': 50,\n",
       " '참': 51,\n",
       " '약': 52,\n",
       " '채': 53,\n",
       " '집': 54,\n",
       " '스': 55,\n",
       " '줄': 56,\n",
       " '몰': 57,\n",
       " '짓': 58,\n",
       " '서': 59,\n",
       " '르': 60,\n",
       " '았': 61,\n",
       " '네': 62,\n",
       " '윷': 63,\n",
       " '색': 64,\n",
       " '금': 65,\n",
       " '길': 66,\n",
       " '독': 67,\n",
       " '늑': 68,\n",
       " '닐': 69,\n",
       " '양': 70,\n",
       " '름': 71,\n",
       " '완': 72,\n",
       " '근': 73,\n",
       " '속': 74,\n",
       " '잘': 75,\n",
       " '학': 76,\n",
       " '벌': 77,\n",
       " '데': 78,\n",
       " '재': 79,\n",
       " '믿': 80,\n",
       " '떻': 81,\n",
       " '조': 82,\n",
       " '가': 83,\n",
       " '불': 84,\n",
       " '칸': 85,\n",
       " '열': 86,\n",
       " '롭': 87,\n",
       " '히': 88,\n",
       " '소': 89,\n",
       " '넌': 90,\n",
       " '질': 91,\n",
       " '난': 92,\n",
       " '욕': 93,\n",
       " '힘': 94,\n",
       " '으': 95,\n",
       " '손': 96,\n",
       " '필': 97,\n",
       " '람': 98,\n",
       " '보': 99,\n",
       " '력': 100,\n",
       " '숴': 101,\n",
       " '뭐': 102,\n",
       " '각': 103,\n",
       " '빈': 104,\n",
       " '갈': 105,\n",
       " '낸': 106,\n",
       " '침': 107,\n",
       " '였': 108,\n",
       " '얘': 109,\n",
       " '예': 110,\n",
       " '담': 111,\n",
       " '임': 112,\n",
       " '노': 113,\n",
       " '었': 114,\n",
       " '염': 115,\n",
       " '활': 116,\n",
       " '머': 117,\n",
       " '깎': 118,\n",
       " '앞': 119,\n",
       " '미': 120,\n",
       " '없': 121,\n",
       " '힌': 122,\n",
       " '얀': 123,\n",
       " '존': 124,\n",
       " '평': 125,\n",
       " '눌': 126,\n",
       " '듣': 127,\n",
       " '버': 128,\n",
       " '죽': 129,\n",
       " '요': 130,\n",
       " '떠': 131,\n",
       " '자': 132,\n",
       " '정': 133,\n",
       " '꾸': 134,\n",
       " '문': 135,\n",
       " '런': 136,\n",
       " '호': 137,\n",
       " '꽤': 138,\n",
       " '뵙': 139,\n",
       " '혀': 140,\n",
       " '쩔': 141,\n",
       " '감': 142,\n",
       " '는': 143,\n",
       " '턱': 144,\n",
       " '른': 145,\n",
       " '강': 146,\n",
       " '만': 147,\n",
       " '림': 148,\n",
       " '찾': 149,\n",
       " '겁': 150,\n",
       " '한': 151,\n",
       " '밖': 152,\n",
       " '좋': 153,\n",
       " '신': 154,\n",
       " '부': 155,\n",
       " '깥': 156,\n",
       " '착': 157,\n",
       " '방': 158,\n",
       " '쓰': 159,\n",
       " '얻': 160,\n",
       " '맞': 161,\n",
       " ' ': 162,\n",
       " '냐': 163,\n",
       " '를': 164,\n",
       " '봤': 165,\n",
       " '터': 166,\n",
       " '코': 167,\n",
       " '용': 168,\n",
       " '수': 169,\n",
       " '청': 170,\n",
       " '듬': 171,\n",
       " '망': 172,\n",
       " '복': 173,\n",
       " '튼': 174,\n",
       " '습': 175,\n",
       " '꺼': 176,\n",
       " '물': 177,\n",
       " '종': 178,\n",
       " '혹': 179,\n",
       " '접': 180,\n",
       " '꼭': 181,\n",
       " '절': 182,\n",
       " '것': 183,\n",
       " '짜': 184,\n",
       " '김': 185,\n",
       " '랑': 186,\n",
       " '통': 187,\n",
       " '키': 188,\n",
       " '하': 189,\n",
       " '온': 190,\n",
       " '녀': 191,\n",
       " '패': 192,\n",
       " '친': 193,\n",
       " '닌': 194,\n",
       " '뒷': 195,\n",
       " '꿍': 196,\n",
       " '먼': 197,\n",
       " '계': 198,\n",
       " '웠': 199,\n",
       " '선': 200,\n",
       " '걸': 201,\n",
       " '엇': 202,\n",
       " '검': 203,\n",
       " '랍': 204,\n",
       " '때': 205,\n",
       " '또': 206,\n",
       " '칭': 207,\n",
       " '란': 208,\n",
       " '혔': 209,\n",
       " '훨': 210,\n",
       " '혜': 211,\n",
       " '까': 212,\n",
       " '탕': 213,\n",
       " '님': 214,\n",
       " '로': 215,\n",
       " '받': 216,\n",
       " '할': 217,\n",
       " '못': 218,\n",
       " '찰': 219,\n",
       " '법': 220,\n",
       " '혼': 221,\n",
       " '내': 222,\n",
       " '께': 223,\n",
       " '쎄': 224,\n",
       " '음': 225,\n",
       " '매': 226,\n",
       " '쏙': 227,\n",
       " '적': 228,\n",
       " '벽': 229,\n",
       " '썼': 230,\n",
       " '걱': 231,\n",
       " '니': 232,\n",
       " '함': 233,\n",
       " '텐': 234,\n",
       " '토': 235,\n",
       " '대': 236,\n",
       " '숭': 237,\n",
       " '돼': 238,\n",
       " '옹': 239,\n",
       " '시': 240,\n",
       " '차': 241,\n",
       " '테': 242,\n",
       " '입': 243,\n",
       " '귀': 244,\n",
       " '괴': 245,\n",
       " '상': 246,\n",
       " '주': 247,\n",
       " '곳': 248,\n",
       " '체': 249,\n",
       " '점': 250,\n",
       " '케': 251,\n",
       " '리': 252,\n",
       " '릴': 253,\n",
       " '넣': 254,\n",
       " '볼': 255,\n",
       " '일': 256,\n",
       " '파': 257,\n",
       " '타': 258,\n",
       " '세': 259,\n",
       " '맺': 260,\n",
       " '창': 261,\n",
       " '처': 262,\n",
       " '언': 263,\n",
       " '디': 264,\n",
       " '궁': 265,\n",
       " '생': 266,\n",
       " '남': 267,\n",
       " '행': 268,\n",
       " '도': 269,\n",
       " '알': 270,\n",
       " '운': 271,\n",
       " '렉': 272,\n",
       " '관': 273,\n",
       " '흥': 274,\n",
       " '녕': 275,\n",
       " '컵': 276,\n",
       " '즈': 277,\n",
       " '개': 278,\n",
       " '과': 279,\n",
       " '저': 280,\n",
       " '앉': 281,\n",
       " '살': 282,\n",
       " '산': 283,\n",
       " '되': 284,\n",
       " '쓴': 285,\n",
       " '굉': 286,\n",
       " '기': 287,\n",
       " '야': 288,\n",
       " '새': 289,\n",
       " '렵': 290,\n",
       " '셨': 291,\n",
       " '연': 292,\n",
       " '두': 293,\n",
       " '흰': 294,\n",
       " '떡': 295,\n",
       " '교': 296,\n",
       " '설': 297,\n",
       " '에': 298,\n",
       " '역': 299,\n",
       " '듯': 300,\n",
       " '책': 301,\n",
       " '험': 302,\n",
       " '쳤': 303,\n",
       " '랫': 304,\n",
       " '라': 305,\n",
       " '숨': 306,\n",
       " '낱': 307,\n",
       " '칠': 308,\n",
       " '깨': 309,\n",
       " '씀': 310,\n",
       " '돈': 311,\n",
       " '후': 312,\n",
       " '드': 313,\n",
       " '농': 314,\n",
       " '구': 315,\n",
       " '왔': 316,\n",
       " '긴': 317,\n",
       " '전': 318,\n",
       " '밀': 319,\n",
       " '던': 320,\n",
       " '따': 321,\n",
       " '표': 322,\n",
       " '사': 323,\n",
       " '흑': 324,\n",
       " '번': 325,\n",
       " '판': 326,\n",
       " '장': 327,\n",
       " '제': 328,\n",
       " '뿐': 329,\n",
       " '국': 330,\n",
       " '죠': 331,\n",
       " '크': 332,\n",
       " '쑥': 333,\n",
       " '심': 334,\n",
       " '치': 335,\n",
       " '쁘': 336,\n",
       " '려': 337,\n",
       " '읗': 338,\n",
       " '풀': 339,\n",
       " '첫': 340,\n",
       " '씨': 341,\n",
       " '땅': 342,\n",
       " '팔': 343,\n",
       " '향': 344,\n",
       " '올': 345,\n",
       " '경': 346,\n",
       " '뻐': 347,\n",
       " '뛰': 348,\n",
       " '준': 349,\n",
       " '러': 350,\n",
       " '견': 351,\n",
       " '목': 352,\n",
       " '많': 353,\n",
       " '렀': 354,\n",
       " '어': 355,\n",
       " '좀': 356,\n",
       " '인': 357,\n",
       " '달': 358,\n",
       " '뻔': 359,\n",
       " '나': 360,\n",
       " '찬': 361,\n",
       " '래': 362,\n",
       " '냥': 363,\n",
       " '그': 364,\n",
       " '단': 365,\n",
       " '빌': 366,\n",
       " '작': 367,\n",
       " '면': 368,\n",
       " '짚': 369,\n",
       " '짝': 370,\n",
       " '랄': 371,\n",
       " '울': 372,\n",
       " '글': 373,\n",
       " '쉬': 374,\n",
       " '읽': 375,\n",
       " '쪽': 376,\n",
       " '갔': 377,\n",
       " '잡': 378,\n",
       " '건': 379,\n",
       " '딱': 380,\n",
       " '막': 381,\n",
       " '공': 382,\n",
       " '엽': 383,\n",
       " '맛': 384,\n",
       " '트': 385,\n",
       " '왜': 386,\n",
       " '져': 387,\n",
       " '북': 388,\n",
       " '뒤': 389,\n",
       " '엄': 390,\n",
       " '카': 391,\n",
       " '등': 392,\n",
       " '을': 393,\n",
       " '족': 394,\n",
       " '너': 395,\n",
       " '햄': 396,\n",
       " '럼': 397,\n",
       " '본': 398,\n",
       " '틀': 399,\n",
       " '씬': 400,\n",
       " '봐': 401,\n",
       " '렇': 402,\n",
       " '늘': 403,\n",
       " '누': 404,\n",
       " '잖': 405,\n",
       " '린': 406,\n",
       " '돌': 407,\n",
       " '황': 408,\n",
       " '째': 409,\n",
       " '거': 410,\n",
       " '빨': 411,\n",
       " '냈': 412,\n",
       " '간': 413,\n",
       " '뭇': 414,\n",
       " '않': 415,\n",
       " '떤': 416,\n",
       " '놀': 417,\n",
       " '즐': 418,\n",
       " '잇': 419,\n",
       " '합': 420,\n",
       " '지': 421,\n",
       " '화': 422,\n",
       " '써': 423,\n",
       " '났': 424,\n",
       " '발': 425,\n",
       " '프': 426,\n",
       " '했': 427,\n",
       " '날': 428,\n",
       " '든': 429,\n",
       " '십': 430,\n",
       " '분': 431,\n",
       " '졌': 432,\n",
       " '풍': 433,\n",
       " '은': 434,\n",
       " '된': 435,\n",
       " '싸': 436,\n",
       " '뿍': 437,\n",
       " '푸': 438,\n",
       " '옷': 439,\n",
       " '냉': 440,\n",
       " '환': 441,\n",
       " '뱃': 442,\n",
       " '배': 443,\n",
       " '겨': 444,\n",
       " '삼': 445,\n",
       " '뼈': 446,\n",
       " '똑': 447,\n",
       " '안': 448,\n",
       " '바': 449,\n",
       " '원': 450,\n",
       " '희': 451,\n",
       " '떨': 452,\n",
       " '있': 453,\n",
       " '여': 454,\n",
       " '씩': 455,\n",
       " '아': 456,\n",
       " '루': 457,\n",
       " '랗': 458,\n",
       " '겠': 459,\n",
       " '끼': 460,\n",
       " '중': 461,\n",
       " '큰': 462,\n",
       " '위': 463,\n",
       " '며': 464,\n",
       " '줘': 465,\n",
       " '와': 466,\n",
       " '먹': 467,\n",
       " '유': 468,\n",
       " '고': 469,\n",
       " '눈': 470,\n",
       " '싶': 471,\n",
       " '억': 472,\n",
       " '얼': 473,\n",
       " '럽': 474,\n",
       " '냠': 475,\n",
       " '무': 476,\n",
       " '쳐': 477,\n",
       " '섯': 478,\n",
       " '초': 479,\n",
       " '몹': 480,\n",
       " '빠': 481,\n",
       " '짐': 482,\n",
       " '될': 483,\n",
       " '읍': 484,\n",
       " '랬': 485,\n",
       " '비': 486,\n",
       " '진': 487,\n",
       " '낌': 488,\n",
       " '편': 489,\n",
       " '쩌': 490,\n",
       " '명': 491}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_dict = {v: k for k, v in enumerate(vocab_list)}\n",
    "vocab_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "_hi_HbR_rCKb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "494"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_dict[\"|\"] = vocab_dict[\" \"]\n",
    "del vocab_dict[\" \"]\n",
    "vocab_dict[\"[UNK]\"] = len(vocab_dict)\n",
    "vocab_dict[\"[PAD]\"] = len(vocab_dict)\n",
    "len(vocab_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "rMWugphlrsZ1"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "with open('vocab.json', 'w') as vocab_file:\n",
    "    json.dump(vocab_dict, vocab_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\sangt\\anaconda3\\lib\\site-packages (4.19.4)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\sangt\\anaconda3\\lib\\site-packages (from transformers) (4.64.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in c:\\users\\sangt\\anaconda3\\lib\\site-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\sangt\\anaconda3\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\sangt\\anaconda3\\lib\\site-packages (from transformers) (1.21.6)\n",
      "Requirement already satisfied: requests in c:\\users\\sangt\\anaconda3\\lib\\site-packages (from transformers) (2.23.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in c:\\users\\sangt\\anaconda3\\lib\\site-packages (from transformers) (0.12.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\sangt\\anaconda3\\lib\\site-packages (from transformers) (21.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\sangt\\anaconda3\\lib\\site-packages (from transformers) (3.3.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\sangt\\anaconda3\\lib\\site-packages (from transformers) (2021.8.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\sangt\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\sangt\\anaconda3\\lib\\site-packages (from packaging>=20.0->transformers) (3.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\sangt\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\sangt\\anaconda3\\lib\\site-packages (from requests->transformers) (1.25.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sangt\\anaconda3\\lib\\site-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\sangt\\anaconda3\\lib\\site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\sangt\\anaconda3\\lib\\site-packages (from requests->transformers) (3.0.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\sangt\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -qdm (c:\\users\\sangt\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -heel (c:\\users\\sangt\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\sangt\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -qdm (c:\\users\\sangt\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -heel (c:\\users\\sangt\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\sangt\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -qdm (c:\\users\\sangt\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -heel (c:\\users\\sangt\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\sangt\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -qdm (c:\\users\\sangt\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -heel (c:\\users\\sangt\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\sangt\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -qdm (c:\\users\\sangt\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -heel (c:\\users\\sangt\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\sangt\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -qdm (c:\\users\\sangt\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -heel (c:\\users\\sangt\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "YJUUl9lnryU-"
   },
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2CTCTokenizer\n",
    "\n",
    "tokenizer = Wav2Vec2CTCTokenizer(\"./vocab.json\",\n",
    "                                 unk_token=\"[UNK]\",\n",
    "                                 pad_token=\"[PAD]\",\n",
    "                                 word_delimiter_token=\"|\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gCtrdRsuuDix"
   },
   "source": [
    "## Create XLSR-Wav2Vec2 Feature Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "O0vdrkhKr8D1"
   },
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2FeatureExtractor\n",
    "\n",
    "feature_extractor = Wav2Vec2FeatureExtractor(feature_size=1,\n",
    "                                             sampling_rate=16000,\n",
    "                                             padding_value=0.0,\n",
    "                                             do_normalize=True,\n",
    "                                             return_attention_mask=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "fY_Qm1dpsE78"
   },
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2Processor\n",
    "\n",
    "processor = Wav2Vec2Processor(feature_extractor=feature_extractor,\n",
    "                              tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Wav2Vec2Processor:\n",
       "- feature_extractor: Wav2Vec2FeatureExtractor {\n",
       "  \"do_normalize\": true,\n",
       "  \"feature_extractor_type\": \"Wav2Vec2FeatureExtractor\",\n",
       "  \"feature_size\": 1,\n",
       "  \"padding_side\": \"right\",\n",
       "  \"padding_value\": 0.0,\n",
       "  \"return_attention_mask\": true,\n",
       "  \"sampling_rate\": 16000\n",
       "}\n",
       "\n",
       "- tokenizer: PreTrainedTokenizer(name_or_path='', vocab_size=494, model_max_len=1000000000000000019884624838656, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '[UNK]', 'pad_token': '[PAD]'})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "KTUahP1PsL3g"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "9tMSA3xKsVma"
   },
   "outputs": [],
   "source": [
    "# processor.save_pretrained(\"/content/gdrive/MyDrive/wav2vec2-large-xlsr-ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MZiv4zqyt2cY"
   },
   "source": [
    "## Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "AscOxzgUsb50"
   },
   "outputs": [],
   "source": [
    "def prepare_dataset(batch):\n",
    "    # batched output is \"un-batched\"\n",
    "    batch[\"input_values\"] = processor(batch[\"array\"], sampling_rate=16000).input_values[0]\n",
    "    \n",
    "    with processor.as_target_processor():\n",
    "        batch[\"labels\"] = processor(batch[\"text\"]).input_ids\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ab7d79b88cb470b9168a6500b256016",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/301 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "order_voice = remove_spectial_char_data.map(\n",
    "    prepare_dataset,\n",
    "    remove_columns=remove_spectial_char_data.column_names,\n",
    "    # num_proc=4 num_proc=4이 옵션은 주피터 랩에서는 안된다. 하드웨어와 연관이 있는 듯? colab에서는 이 옵션을 추가해도 된다.\n",
    "#     # 병렬 처리에 관련된 옵션이라고 한다.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "StCtCQxDtuY3"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nUTqiRY-tpXZ"
   },
   "source": [
    "## Set-up Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "nTlewiOzsyqR"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Any, Dict, List, Optional, Union\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorCTCWithPadding:\n",
    "    \"\"\"\n",
    "    Data collator that will dynamically pad the inputs received.\n",
    "    Args:\n",
    "        processor (:class:`~transformers.Wav2Vec2Processor`)\n",
    "            The processor used for proccessing the data.\n",
    "        padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):\n",
    "            Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n",
    "            among:\n",
    "            * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
    "              sequence if provided).\n",
    "            * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n",
    "              maximum acceptable input length for the model if that argument is not provided.\n",
    "            * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n",
    "              different lengths).\n",
    "        max_length (:obj:`int`, `optional`):\n",
    "            Maximum length of the ``input_values`` of the returned list and optionally padding length (see above).\n",
    "        max_length_labels (:obj:`int`, `optional`):\n",
    "            Maximum length of the ``labels`` returned list and optionally padding length (see above).\n",
    "        pad_to_multiple_of (:obj:`int`, `optional`):\n",
    "            If set will pad the sequence to a multiple of the provided value.\n",
    "            This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability >=\n",
    "            7.5 (Volta).\n",
    "    \"\"\"\n",
    "\n",
    "    processor: Wav2Vec2Processor\n",
    "    padding: Union[bool, str] = True\n",
    "    max_length: Optional[int] = None\n",
    "    max_length_labels: Optional[int] = None\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "    pad_to_multiple_of_labels: Optional[int] = None\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lenghts and need\n",
    "        # different padding methods\n",
    "        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "\n",
    "        batch = self.processor.pad(\n",
    "            input_features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        with self.processor.as_target_processor():\n",
    "            labels_batch = self.processor.pad(\n",
    "                label_features,\n",
    "                padding=self.padding,\n",
    "                max_length=self.max_length_labels,\n",
    "                pad_to_multiple_of=self.pad_to_multiple_of_labels,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "WmfIPs18tGQb"
   },
   "outputs": [],
   "source": [
    "data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting jiwer\n",
      "  Using cached jiwer-2.3.0-py3-none-any.whl (15 kB)\n",
      "Collecting python-Levenshtein==0.12.2\n",
      "  Using cached python-Levenshtein-0.12.2.tar.gz (50 kB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\sangt\\anaconda3\\envs\\stt\\lib\\site-packages (from python-Levenshtein==0.12.2->jiwer) (61.2.0)\n",
      "Building wheels for collected packages: python-Levenshtein\n",
      "  Building wheel for python-Levenshtein (setup.py): started\n",
      "  Building wheel for python-Levenshtein (setup.py): finished with status 'done'\n",
      "  Created wheel for python-Levenshtein: filename=python_Levenshtein-0.12.2-cp37-cp37m-win_amd64.whl size=80836 sha256=d42acc2d29eda69da12694f848bc370b10f9995854a1208fc78b004b2d12766c\n",
      "  Stored in directory: c:\\users\\sangt\\appdata\\local\\pip\\cache\\wheels\\05\\5f\\ca\\7c4367734892581bb5ff896f15027a932c551080b2abd3e00d\n",
      "Successfully built python-Levenshtein\n",
      "Installing collected packages: python-Levenshtein, jiwer\n",
      "Successfully installed jiwer-2.3.0 python-Levenshtein-0.12.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install jiwer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "vJjfNy9-tJM6"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_metric, Audio\n",
    "\n",
    "wer_metric = load_metric(\"wer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "03mNNUfNthSZ"
   },
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    pred_logits = pred.predictions\n",
    "    pred_ids = np.argmax(pred_logits, axis=-1)\n",
    "\n",
    "    pred.label_ids[pred.label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "\n",
    "    pred_str = processor.batch_decode(pred_ids)\n",
    "    # we do not want to group tokens when computing the metrics\n",
    "    label_str = processor.batch_decode(pred.label_ids, group_tokens=False)\n",
    "\n",
    "    wer = wer_metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return {\"wer\": wer}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4vgRoeQHvTqk"
   },
   "source": [
    "## Import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "Iyv4qaclue6n"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8540a79fa16a4439846d052dab79e608",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.73k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dc11d116a714f2a9ae0f703dc4ef21c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.18G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/wav2vec2-large-xlsr-53 were not used when initializing Wav2Vec2ForCTC: ['quantizer.codevectors', 'quantizer.weight_proj.bias', 'project_hid.weight', 'project_q.weight', 'quantizer.weight_proj.weight', 'project_hid.bias', 'project_q.bias']\n",
      "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-large-xlsr-53 and are newly initialized: ['lm_head.weight', 'lm_head.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import Wav2Vec2ForCTC\n",
    "\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\n",
    "    \"facebook/wav2vec2-large-xlsr-53\", \n",
    "    attention_dropout=0.1,\n",
    "    hidden_dropout=0.1,\n",
    "    feat_proj_dropout=0.0,\n",
    "    mask_time_prob=0.05,\n",
    "    layerdrop=0.1,\n",
    "    ctc_loss_reduction=\"mean\", \n",
    "    pad_token_id=processor.tokenizer.pad_token_id,\n",
    "    vocab_size=len(processor.tokenizer)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MaYmsSutuv26"
   },
   "source": [
    "XLSR-Wav2Vec2의 첫 번째 구성 요소는 원시 음성 신호에서 음향적으로 의미가 있지만 문맥적으로 독립적인 기능을 추출하는 데 사용되는 CNN 계층 스택으로 구성됩니다.  \n",
    "모델의 이 부분은 사전 교육 중에 이미 충분히 훈련되었으며 논문에 명시된 바와 같이 더 이상 미세 조정할 필요가 없습니다. 따라서 특징 추출 부분의 모든 파라미터에 대해 require_grad를 False로 설정할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "5VX8z6PpuhqG"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sangt\\anaconda3\\envs\\STT\\lib\\site-packages\\transformers\\models\\wav2vec2\\modeling_wav2vec2.py:1677: FutureWarning: The method `freeze_feature_extractor` is deprecated and will be removed in Transformers v5.Please use the equivalent `freeze_feature_encoder` method instead.\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "model.freeze_feature_extractor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wAfoHUk9vGxd"
   },
   "source": [
    "메모리를 절약하기 위해 그라데이션 체크포인팅을 활성화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "wZm6in_2vAIN"
   },
   "outputs": [],
   "source": [
    "model.gradient_checkpointing_enable()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h-aJNQplvQgn"
   },
   "source": [
    "## TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "-LuuwoeMvOnV"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Mixed precision training with AMP or APEX (`--fp16` or `--bf16`) and half precision evaluation (`--fp16_full_eval` or `--bf16_full_eval`) can only be used on CUDA devices.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_1640\\3184135767.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m   \u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3e-4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m   \u001b[0mwarmup_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m500\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m   \u001b[0msave_total_limit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m )\n",
      "\u001b[1;32m~\\anaconda3\\envs\\STT\\lib\\site-packages\\transformers\\training_args.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, output_dir, overwrite_output_dir, do_train, do_eval, do_predict, evaluation_strategy, prediction_loss_only, per_device_train_batch_size, per_device_eval_batch_size, per_gpu_train_batch_size, per_gpu_eval_batch_size, gradient_accumulation_steps, eval_accumulation_steps, eval_delay, learning_rate, weight_decay, adam_beta1, adam_beta2, adam_epsilon, max_grad_norm, num_train_epochs, max_steps, lr_scheduler_type, warmup_ratio, warmup_steps, log_level, log_level_replica, log_on_each_node, logging_dir, logging_strategy, logging_first_step, logging_steps, logging_nan_inf_filter, save_strategy, save_steps, save_total_limit, save_on_each_node, no_cuda, seed, data_seed, bf16, fp16, fp16_opt_level, half_precision_backend, bf16_full_eval, fp16_full_eval, tf32, local_rank, xpu_backend, tpu_num_cores, tpu_metrics_debug, debug, dataloader_drop_last, eval_steps, dataloader_num_workers, past_index, run_name, disable_tqdm, remove_unused_columns, label_names, load_best_model_at_end, metric_for_best_model, greater_is_better, ignore_data_skip, sharded_ddp, fsdp, fsdp_min_num_params, deepspeed, label_smoothing_factor, optim, adafactor, group_by_length, length_column_name, report_to, ddp_find_unused_parameters, ddp_bucket_cap_mb, dataloader_pin_memory, skip_memory_metrics, use_legacy_prediction_loop, push_to_hub, resume_from_checkpoint, hub_model_id, hub_strategy, hub_token, hub_private_repo, gradient_checkpointing, include_inputs_for_metrics, fp16_backend, push_to_hub_model_id, push_to_hub_organization, push_to_hub_token, mp_parameters, auto_find_batch_size, full_determinism)\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\STT\\lib\\site-packages\\transformers\\training_args.py\u001b[0m in \u001b[0;36m__post_init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    936\u001b[0m         ):\n\u001b[0;32m    937\u001b[0m             raise ValueError(\n\u001b[1;32m--> 938\u001b[1;33m                 \u001b[1;34m\"Mixed precision training with AMP or APEX (`--fp16` or `--bf16`) and half precision evaluation (`--fp16_full_eval` or `--bf16_full_eval`) can only be used on CUDA devices.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    939\u001b[0m             )\n\u001b[0;32m    940\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Mixed precision training with AMP or APEX (`--fp16` or `--bf16`) and half precision evaluation (`--fp16_full_eval` or `--bf16_full_eval`) can only be used on CUDA devices."
     ]
    }
   ],
   "source": [
    "# cuda 장비 없으면 에러 발생 \n",
    "\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "  # output_dir=\"/content/gdrive/MyDrive/wav2vec2-large-xlsr-turkish-demo\",\n",
    "  output_dir=\"./wav2vec2-large-xlsr-turkish-demo\",\n",
    "  group_by_length=True,\n",
    "  per_device_train_batch_size=16,\n",
    "  gradient_accumulation_steps=2,\n",
    "  evaluation_strategy=\"steps\",\n",
    "  num_train_epochs=3,\n",
    "  fp16=True,\n",
    "  save_steps=100,\n",
    "  eval_steps=100,\n",
    "  logging_steps=10,\n",
    "  learning_rate=3e-4,\n",
    "  warmup_steps=500,\n",
    "  save_total_limit=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6vhFC0BdvcOu"
   },
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    data_collator=data_collator,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=order_voice, # train_ds\n",
    "    tokenizer=processor.feature_extractor,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aHGNj1NSvsFk"
   },
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator(remove_spectial_char_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a-rJ8fokvjJb"
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FxpIR_X8w3Pq"
   },
   "source": [
    "CTC 손실을 사용하여 더 큰 데이터 세트에서 더 큰 모델을 미세 조정하려면 [여기서](https://github.com/huggingface/transformers/tree/master/examples/pytorch/speech-recognition#connectionist-temporal-classification-without-language-model-ctc-wo-lm) 공식 음성 인식 예를 살펴봐야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "shZn6QzW0T1h"
   },
   "source": [
    "## Model predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "umACSYoA0SV7"
   },
   "outputs": [],
   "source": [
    "model(audio_data)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "00.XLSR - Wav2Vec2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "STT",
   "language": "python",
   "name": "stt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "19631f30805cf65d5465564d75f0fe7c05dee5c1f7be198222dbe754da644e52"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
